{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9095,
     "status": "ok",
     "timestamp": 1759138388419,
     "user": {
      "displayName": "Ingo Stallknecht",
      "userId": "04299073505858791564"
     },
     "user_tz": -120
    },
    "id": "U0oczh-teCiN",
    "outputId": "1e690a21-4990-4de9-d7bf-7a7e095dc237"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/717.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m716.8/717.0 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m717.0/717.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.2/17.2 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.6/507.6 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for odfpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# ===== 0) Install (run once) =====\n",
    "!pip -q install geopandas folium matplotlib-scalebar pyogrio shapely fiona openpyxl odfpy branca rtree pyproj\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22636,
     "status": "ok",
     "timestamp": 1759138411058,
     "user": {
      "displayName": "Ingo Stallknecht",
      "userId": "04299073505858791564"
     },
     "user_tz": -120
    },
    "id": "E3u48Y9veHA_",
    "outputId": "0f91f12e-3d45-4c4f-866d-184264737e00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# ===== 1) Imports & global config =====\n",
    "import os, io, re, json, time, unicodedata as ucn\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyArrow\n",
    "import folium\n",
    "import branca.colormap as bcm\n",
    "\n",
    "try:\n",
    "    from matplotlib_scalebar.scalebar import ScaleBar\n",
    "    HAS_SCALEBAR = True\n",
    "except Exception:\n",
    "    HAS_SCALEBAR = False\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "\n",
    "# Colab Drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# ---- Paths (edit DATA_DIR only) ----\n",
    "DATA_DIR   = \"/content/drive/MyDrive/data\"   # << change if needed\n",
    "VACANCY_CSV = os.path.join(DATA_DIR, \"vacancy_municipality.csv\")\n",
    "GTFS_ZIP   = os.path.join(DATA_DIR, \"gtfs_train.zip\")\n",
    "OUT_DIR    = \"/content\"\n",
    "ARTIFACTS  = os.path.join(OUT_DIR, \"artifacts\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(ARTIFACTS, exist_ok=True)\n",
    "\n",
    "# ---- Reproducibility ----\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ---- CRS constants ----\n",
    "CRS_WGS84 = \"EPSG:4326\"\n",
    "CRS_CH    = \"EPSG:2056\"   # Swiss projected (meters)\n",
    "\n",
    "# Small helper to persist quick PNGs\n",
    "def _savefig(path):\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, bbox_inches=\"tight\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1759138411060,
     "user": {
      "displayName": "Ingo Stallknecht",
      "userId": "04299073505858791564"
     },
     "user_tz": -120
    },
    "id": "cljLFDAZeKFG"
   },
   "outputs": [],
   "source": [
    "# ===== 2) Helpers =====\n",
    "def norm_name(s: str) -> str:\n",
    "    \"\"\"Normalize municipality names for joining.\"\"\"\n",
    "    if pd.isna(s): return s\n",
    "    s = (str(s)\n",
    "         .replace(\"ä\",\"ae\").replace(\"ö\",\"oe\").replace(\"ü\",\"ue\")\n",
    "         .replace(\"Ä\",\"ae\").replace(\"Ö\",\"oe\").replace(\"Ü\",\"ue\")\n",
    "         .replace(\"ß\",\"ss\"))\n",
    "    s = ucn.normalize(\"NFKD\", s).encode(\"ascii\",\"ignore\").decode(\"ascii\")\n",
    "    s = re.sub(r\"[^A-Za-z0-9]+\",\"\", s).lower()\n",
    "    return s\n",
    "\n",
    "def read_bfs_like_csv(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Read BFS SDMX-like CSV with semicolons; auto-detect header line.\"\"\"\n",
    "    text = None\n",
    "    for enc in (\"utf-8-sig\",\"utf-8\",\"latin1\"):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=enc) as f:\n",
    "                text = f.read()\n",
    "            break\n",
    "        except Exception:\n",
    "            pass\n",
    "    if text is None:\n",
    "        raise IOError(f\"Cannot read file {path} with utf-8/latin1.\")\n",
    "    lines = text.splitlines()\n",
    "    hdr_idx, pat = 0, re.compile(r\"(TIME_PERIOD|Zeitperiode).*(OBS_VALUE|Beobachtungswert)\")\n",
    "    for i, ln in enumerate(lines[:200]):\n",
    "        if pat.search(ln): hdr_idx = i; break\n",
    "    return pd.read_csv(io.StringIO(\"\\n\".join(lines[hdr_idx:])), sep=\";\", engine=\"python\", dtype=str)\n",
    "\n",
    "def parse_percent(x):\n",
    "    \"\"\"Parse text to decimal percent (e.g., '0.51' -> 0.51%).\"\"\"\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)): return np.nan\n",
    "    s = str(x).strip()\n",
    "    if not s: return np.nan\n",
    "    s = s.replace(\"\\xa0\",\" \").replace(\"%\",\"\").strip()\n",
    "    s = s.replace(\"’\",\"\").replace(\"'\",\"\").replace(\" \", \"\").replace(\",\", \".\")\n",
    "    if re.search(r\"\\bjan\\b\", s, flags=re.I):  # Excel oddities (rare)\n",
    "        nums = re.findall(r\"(\\d+)\", s)\n",
    "        return float(nums[0]) / 100.0 if nums else np.nan\n",
    "    try:\n",
    "        v = float(s)\n",
    "    except:\n",
    "        return np.nan\n",
    "    return v/100.0 if v > 10 else v  # autoscale when 51 -> 0.51\n",
    "\n",
    "def _key_str(x): return str(x).strip() if pd.notna(x) else x\n",
    "\n",
    "def timer(msg: str):\n",
    "    t0 = time.time()\n",
    "    print(f\"[start] {msg}\")\n",
    "    def _end():\n",
    "        dt = time.time() - t0\n",
    "        print(f\"[done ] {msg} in {dt:.2f}s\")\n",
    "    return _end\n",
    "\n",
    "def assert_cols(df, cols: List[str], name=\"df\"):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    assert not missing, f\"{name} missing columns: {missing}\"\n",
    "\n",
    "def robust_range(series, qlo=1, qhi=99) -> Tuple[float,float]:\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    s = s[np.isfinite(s)]\n",
    "    if s.empty: return (0.0, 1.0)\n",
    "    lo = float(np.nanpercentile(s, qlo))\n",
    "    hi = float(np.nanpercentile(s, qhi))\n",
    "    if not np.isfinite(lo) or not np.isfinite(hi) or hi <= lo:\n",
    "        lo, hi = float(np.nanmin(s)), float(np.nanmax(s))\n",
    "    if not np.isfinite(lo) or not np.isfinite(hi) or hi <= lo:\n",
    "        lo, hi = 0.0, 1.0\n",
    "    return lo, hi\n",
    "\n",
    "def norm01(x, lo, hi):\n",
    "    x = np.asarray(x, dtype=\"float64\")\n",
    "    lo, hi = float(lo), float(hi)\n",
    "    if hi <= lo + 1e-9: return np.zeros_like(x)\n",
    "    return (x - lo) / (hi - lo)\n",
    "\n",
    "def exp_penalty_monotone(t_norm, k):\n",
    "    \"\"\"Convex penalty in [0,1]; larger k -> stronger penalty on long commutes.\"\"\"\n",
    "    t = np.clip(np.asarray(t_norm, dtype=\"float64\"), 0.0, 1.0)\n",
    "    k = float(k)\n",
    "    if k <= 1e-6: return t\n",
    "    return (1.0 - np.exp(-k * t)) / (1.0 - np.exp(-k))\n",
    "\n",
    "def safe_to_crs(gdf, crs):\n",
    "    g = gdf.copy()\n",
    "    try:\n",
    "        if g.crs is None:\n",
    "            return g.set_crs(crs)\n",
    "        elif str(g.crs).lower() != str(crs).lower():\n",
    "            return g.to_crs(crs)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66885,
     "status": "ok",
     "timestamp": 1759138477947,
     "user": {
      "displayName": "Ingo Stallknecht",
      "userId": "04299073505858791564"
     },
     "user_tz": -120
    },
    "id": "2Jj0nj1reMT1",
    "outputId": "cc3c6567-c14d-4c6d-8761-f48034760ff5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[start] Load Swiss Gemeinde geometries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pyogrio._io:Skipping field geo_point_2d: unsupported OGR type: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[done ] Load Swiss Gemeinde geometries in 66.81s\n",
      "Gemeinden: 2128 | year: 2025\n"
     ]
    }
   ],
   "source": [
    "# ===== 3) Swiss municipalities (latest geometries) =====\n",
    "URL_GEM = (\"https://data.opendatasoft.com/explore/dataset/\"\n",
    "           \"georef-switzerland-gemeinde-millesime%40public/download/\"\n",
    "           \"?format=geojson&timezone=Europe%2FBerlin\")\n",
    "\n",
    "t_end = timer(\"Load Swiss Gemeinde geometries\")\n",
    "gdf_all = gpd.read_file(URL_GEM)[[\"gem_name\",\"gem_code\",\"kan_code\",\"year\",\"geometry\"]].rename(\n",
    "    columns={\"gem_name\":\"GEMEINDE_NAME\",\"gem_code\":\"GEMEINDE_CODE\",\"kan_code\":\"KANTON_CODE\"}\n",
    ")\n",
    "latest_geom_year = gdf_all[\"year\"].max()\n",
    "gdf = (gdf_all[gdf_all[\"year\"] == latest_geom_year]\n",
    "       .sort_values([\"GEMEINDE_CODE\",\"year\"])\n",
    "       .drop_duplicates(\"GEMEINDE_CODE\", keep=\"last\")\n",
    "       .copy())\n",
    "gdf = safe_to_crs(gdf.set_geometry(gdf.geometry.buffer(0)), CRS_CH)\n",
    "gdf[\"area_km2\"] = gdf.area / 1e6\n",
    "t_end()\n",
    "\n",
    "print(\"Gemeinden:\", len(gdf), \"| year:\", latest_geom_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 754,
     "status": "ok",
     "timestamp": 1759138478707,
     "user": {
      "displayName": "Ingo Stallknecht",
      "userId": "04299073505858791564"
     },
     "user_tz": -120
    },
    "id": "wjQc7J63VPDR",
    "outputId": "91d58f26-316c-4a27-b6f7-93ea734e8960"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[start] Load BFS vacancy indicator\n",
      "[done ] Load BFS vacancy indicator in 0.74s\n",
      "Vacancy coverage: 0.995 (2106/2116)\n"
     ]
    }
   ],
   "source": [
    "# ===== 4) BFS vacancy indicator (clean, join, QA) =====\n",
    "t_end = timer(\"Load BFS vacancy indicator\")\n",
    "df_raw = read_bfs_like_csv(VACANCY_CSV)\n",
    "t_end()\n",
    "\n",
    "COL_GEM   = \"Grossregionen, Kantone, Bezirke und Gemeinden\"\n",
    "COL_ROOMS = \"Anzahl Zimmer\"\n",
    "COL_TYPE  = \"Typ der leer stehenden Wohnung\"\n",
    "COL_MEAS  = \"Art der Messung\"\n",
    "COL_VAL   = \"OBS_VALUE\" if \"OBS_VALUE\" in df_raw.columns else \"Beobachtungswert\"\n",
    "\n",
    "df = df_raw.copy()\n",
    "df[\"key_name\"] = df[COL_GEM].astype(str).str.strip()\n",
    "\n",
    "m_rooms = df[COL_ROOMS].astype(str).str.strip().str.lower().isin([\"total\",\"_t\",\"gesamt\"])\n",
    "m_type  = df[COL_TYPE ].astype(str).str.lower().str.contains(\"alle\", case=False)\n",
    "m_meas  = df[COL_MEAS ].astype(str).str.lower().str.contains(\"anteil\", case=False)\n",
    "df = df[m_rooms & m_type & m_meas].copy()\n",
    "df[\"vacancy_pct\"] = df[COL_VAL].map(parse_percent)\n",
    "\n",
    "NAME_MAP_RAW = {\n",
    "    \"Biel\": \"Biel/Bienne\", \"Bienne\": \"Biel/Bienne\",\n",
    "    \"Leubringen\": \"Evilard\",\n",
    "    \"Saint Imier\": \"Saint-Imier\", \"St Imier\": \"Saint-Imier\", \"St-Imier\": \"Saint-Imier\",\n",
    "    \"Münster (BE)\": \"Moutier\",\n",
    "}\n",
    "df[\"key_name_fix\"] = df[\"key_name\"].map(lambda s: NAME_MAP_RAW.get(str(s).strip(), s))\n",
    "\n",
    "ind = (df.groupby(\"key_name_fix\", as_index=False)[\"vacancy_pct\"].mean()\n",
    "         .rename(columns={\"key_name_fix\":\"key_name\"}))\n",
    "\n",
    "gdf[\"key_norm\"] = gdf[\"GEMEINDE_NAME\"].map(norm_name)\n",
    "ind[\"key_norm\"] = ind[\"key_name\"].map(norm_name)\n",
    "g = gdf.merge(ind[[\"key_norm\",\"vacancy_pct\"]], on=\"key_norm\", how=\"left\")\n",
    "\n",
    "# Drop known non-municipals\n",
    "NON_MUNI_EXACT = {\n",
    "    \"Zürichsee (ZH)\",\"Thunersee\",\"Brienzersee\",\"Bielersee (BE)\",\"Bielersee (NE)\",\n",
    "    \"Lac de Neuchâtel (BE)\",\"Lac de Neuchâtel (NE)\",\"Bodensee (SG)\",\"Bodensee (TG)\",\"Staatswald Galm\"\n",
    "}\n",
    "mask_non = g[\"GEMEINDE_NAME\"].isin(NON_MUNI_EXACT) | g[\"GEMEINDE_NAME\"].str.contains(r\"^Comunanza\\b\", case=False, na=False)\n",
    "g = g.loc[~mask_non].copy()\n",
    "\n",
    "print(\"Vacancy coverage:\", g[\"vacancy_pct\"].notna().mean().round(3), f\"({g['vacancy_pct'].notna().sum()}/{len(g)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23565,
     "status": "ok",
     "timestamp": 1759138502274,
     "user": {
      "displayName": "Ingo Stallknecht",
      "userId": "04299073505858791564"
     },
     "user_tz": -120
    },
    "id": "TS7mHHzGeOw-",
    "outputId": "fdc1c4fa-126e-44f2-d70b-182c4afa5efd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-4106543051.py:9: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  trips      = pd.read_csv(\"/content/gtfs/trips.txt\")\n",
      "/tmp/ipython-input-4106543051.py:10: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  routes     = pd.read_csv(\"/content/gtfs/routes.txt\")\n",
      "/tmp/ipython-input-4106543051.py:11: DtypeWarning: Columns (3,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  stop_times = pd.read_csv(\"/content/gtfs/stop_times.txt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station graph: 9526 edges | stations: 4771\n"
     ]
    }
   ],
   "source": [
    "# ===== 5) GTFS (all PT modes) → station graph with realism guards =====\n",
    "import zipfile\n",
    "\n",
    "if not os.path.exists(\"/content/gtfs\"):\n",
    "    with zipfile.ZipFile(GTFS_ZIP, \"r\") as z:\n",
    "        z.extractall(\"/content/gtfs\")\n",
    "\n",
    "stops      = pd.read_csv(\"/content/gtfs/stops.txt\")\n",
    "trips      = pd.read_csv(\"/content/gtfs/trips.txt\")\n",
    "routes     = pd.read_csv(\"/content/gtfs/routes.txt\")\n",
    "stop_times = pd.read_csv(\"/content/gtfs/stop_times.txt\")\n",
    "\n",
    "# Use ALL modes (tram, metro, rail, bus, ferry, cable, gondola, funicular)\n",
    "USE_ROUTE_TYPES = {0,1,2,3,4,5,6,7}\n",
    "sel_routes = routes[routes[\"route_type\"].isin(USE_ROUTE_TYPES)][\"route_id\"].unique()\n",
    "sel_trips  = trips[trips[\"route_id\"].isin(sel_routes)][\"trip_id\"].unique()\n",
    "st_times   = stop_times[stop_times[\"trip_id\"].isin(sel_trips)].copy()\n",
    "\n",
    "def hms_to_sec(x: str) -> int:\n",
    "    h, m, s = str(x).split(\":\")\n",
    "    return int(h)*3600 + int(m)*60 + int(s)\n",
    "\n",
    "# Map to station_id (parent_station fallback)\n",
    "stops[\"station_id\"] = stops[\"parent_station\"].fillna(stops[\"stop_id\"]).astype(str)\n",
    "st_times[\"stop_id\"] = st_times[\"stop_id\"].astype(str)\n",
    "st_times = st_times.merge(stops[[\"stop_id\",\"station_id\"]], on=\"stop_id\", how=\"left\")\n",
    "\n",
    "st = st_times.dropna(subset=[\"arrival_time\",\"departure_time\",\"station_id\"]).copy()\n",
    "st[\"arr_s\"] = st[\"arrival_time\"].map(hms_to_sec)\n",
    "st[\"dep_s\"] = st[\"departure_time\"].map(hms_to_sec)\n",
    "st.sort_values([\"trip_id\",\"stop_sequence\"], inplace=True)\n",
    "st[\"next_station\"] = st.groupby(\"trip_id\")[\"station_id\"].shift(-1)\n",
    "st[\"next_arr_s\"]   = st.groupby(\"trip_id\")[\"arr_s\"].shift(-1)\n",
    "\n",
    "edges_raw = st.dropna(subset=[\"next_station\",\"next_arr_s\"])[[\"station_id\",\"next_station\",\"dep_s\",\"next_arr_s\"]].copy()\n",
    "edges_raw[\"w_sec\"] = (edges_raw[\"next_arr_s\"] - edges_raw[\"dep_s\"]).astype(float)\n",
    "edge_min = (edges_raw.groupby([\"station_id\",\"next_station\"], as_index=False)[\"w_sec\"]\n",
    "                     .min()\n",
    "                     .rename(columns={\"station_id\":\"u\",\"next_station\":\"v\"}))\n",
    "edge_min[\"u\"] = edge_min[\"u\"].astype(str); edge_min[\"v\"] = edge_min[\"v\"].astype(str)\n",
    "\n",
    "# Station points (EPSG:2056)\n",
    "df_pts = stops[[\"station_id\",\"stop_lon\",\"stop_lat\"]].dropna().drop_duplicates().copy()\n",
    "df_pts[\"stop_lon\"] = pd.to_numeric(df_pts[\"stop_lon\"], errors=\"coerce\")\n",
    "df_pts[\"stop_lat\"] = pd.to_numeric(df_pts[\"stop_lat\"], errors=\"coerce\")\n",
    "df_pts = df_pts.dropna(subset=[\"stop_lon\",\"stop_lat\"]).reset_index(drop=True)\n",
    "\n",
    "stations_g = gpd.GeoDataFrame(\n",
    "    df_pts, geometry=gpd.points_from_xy(df_pts[\"stop_lon\"], df_pts[\"stop_lat\"]), crs=CRS_WGS84\n",
    ").to_crs(CRS_CH)\n",
    "stations_pts = stations_g.dissolve(by=\"station_id\", as_index=False)\n",
    "stations_pts[\"geometry\"] = stations_pts.geometry.centroid\n",
    "stxy = stations_pts.set_index(\"station_id\")[\"geometry\"].apply(lambda p: (p.x, p.y)).to_dict()\n",
    "\n",
    "# Edge sanitization: min time 20s, max edge speed 160 km/h\n",
    "def sanitize_edges(edges_df, stxy_dict, min_sec=20.0, vmax_kmh=160.0):\n",
    "    u_xy = edges_df[\"u\"].map(stxy_dict); v_xy = edges_df[\"v\"].map(stxy_dict)\n",
    "    dx = np.array([a[0]-b[0] if (a and b) else np.nan for a,b in zip(u_xy, v_xy)])\n",
    "    dy = np.array([a[1]-b[1] if (a and b) else np.nan for a,b in zip(u_xy, v_xy)])\n",
    "    dist_m = np.hypot(dx, dy)\n",
    "    w = edges_df[\"w_sec\"].astype(float).to_numpy()\n",
    "    w = np.where(~np.isfinite(w) | (w < min_sec), min_sec, w)\n",
    "    vmax_mps = vmax_kmh/3.6\n",
    "    bad = (dist_m > 0) & (dist_m / w > vmax_mps)\n",
    "    w[bad] = np.maximum(dist_m[bad] / vmax_mps, min_sec)\n",
    "    out = edges_df.copy(); out[\"w_sec\"] = w\n",
    "    return out.dropna(subset=[\"w_sec\"])\n",
    "\n",
    "edge_sane = sanitize_edges(edge_min, stxy)\n",
    "\n",
    "# Adjacency\n",
    "from collections import defaultdict\n",
    "adj = defaultdict(list)\n",
    "for r in edge_sane.itertuples(index=False):\n",
    "    adj[r.u].append((r.v, float(r.w_sec)))\n",
    "\n",
    "print(\"Station graph:\", len(edge_sane), \"edges | stations:\", len(stations_pts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1759138502293,
     "user": {
      "displayName": "Ingo Stallknecht",
      "userId": "04299073505858791564"
     },
     "user_tz": -120
    },
    "id": "LRfQKAjMeRlk"
   },
   "outputs": [],
   "source": [
    "# ===== REPLACE: compute_commute_minutes_from (fixed length-safe smoothing) =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Tunables (same semantics as before)\n",
    "K_NEAREST_ST      = 5        # stations per Gemeinde for soft-min\n",
    "WALK_KMPH         = 5.0\n",
    "WALK_M_PER_MIN    = (WALK_KMPH * 1000) / 60.0\n",
    "MAX_WALK_M        = 15000\n",
    "SOFTMIN_TAU_MIN   = 8.0\n",
    "SMOOTH_SIGMA_M    = 20000.0  # ~20 km smoothing kernel\n",
    "VMAX_GLOBAL_KMH   = 160.0\n",
    "BOARD_MIN         = 4.0\n",
    "\n",
    "def _union_all_geoms(geoms):\n",
    "    \"\"\"Shapely 2.x: union_all; fallback to unary_union.\"\"\"\n",
    "    try:\n",
    "        from shapely import union_all as _u\n",
    "        return _u(list(geoms))\n",
    "    except Exception:\n",
    "        from shapely.ops import unary_union as _uu\n",
    "        return _uu(list(geoms))\n",
    "\n",
    "def _normtxt(s: str) -> str:\n",
    "    return (str(s).lower()\n",
    "            .replace(\"ä\",\"a\").replace(\"ö\",\"o\").replace(\"ü\",\"u\")\n",
    "            .replace(\"é\",\"e\").replace(\"è\",\"e\").replace(\"ê\",\"e\").strip())\n",
    "\n",
    "def match_station_ids_by_name(name: str):\n",
    "    \"\"\"Prefer exact normalized name, else prefix, else substring.\"\"\"\n",
    "    name_map = (stops.groupby(\"station_id\")[\"stop_name\"]\n",
    "                      .agg(lambda s: s.value_counts().idxmax()))\n",
    "    n0 = _normtxt(name)\n",
    "    exact = [sid for sid, nm in name_map.items() if isinstance(nm,str) and _normtxt(nm)==n0]\n",
    "    if exact: return sorted({str(x) for x in exact})\n",
    "    starts = [sid for sid, nm in name_map.items() if isinstance(nm,str) and _normtxt(nm).startswith(n0)]\n",
    "    if starts: return sorted({str(x) for x in starts})\n",
    "    contains = [sid for sid, nm in name_map.items() if isinstance(nm,str) and n0 in _normtxt(nm)]\n",
    "    return sorted({str(x) for x in contains})\n",
    "\n",
    "def dijkstra_from_sources(src_ids):\n",
    "    \"\"\"Multi-source Dijkstra on the station graph. Requires global `adj`.\"\"\"\n",
    "    import heapq\n",
    "    INF = 10**12\n",
    "    dist = {sid: 0.0 for sid in src_ids}\n",
    "    pq = [(0.0, sid) for sid in src_ids]\n",
    "    heapq.heapify(pq)\n",
    "    seen = set()\n",
    "    while pq:\n",
    "        d,u = heapq.heappop(pq)\n",
    "        if u in seen:\n",
    "            continue\n",
    "        seen.add(u)\n",
    "        for v,w in adj.get(u, []):\n",
    "            nd = d + w\n",
    "            if nd < dist.get(v, INF):\n",
    "                dist[v] = nd\n",
    "                heapq.heappush(pq, (nd, v))\n",
    "    return pd.DataFrame({\"station_id\": list(dist.keys()), \"time_sec_station\": list(dist.values())})\n",
    "\n",
    "def _ensure_lv95(gdf):\n",
    "    \"\"\"Ensure LV95 (EPSG:2056) for spatial math.\"\"\"\n",
    "    if gdf.crs is None:\n",
    "        try:\n",
    "            gdf = gdf.set_crs(2056)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if str(gdf.crs).lower() != \"epsg:2056\":\n",
    "        gdf = gdf.to_crs(2056)\n",
    "    return gdf\n",
    "\n",
    "def compute_commute_minutes_from(origin_name: str, g_polys: gpd.GeoDataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute per-Gemeinde minutes from origin_name with realism guards.\n",
    "    Inputs:\n",
    "      - origin_name: station name to match (e.g., 'Zürich HB')\n",
    "      - g_polys: GeoDataFrame in LV95 with ['GEMEINDE_CODE','geometry'].\n",
    "    Returns:\n",
    "      - DataFrame ['GEMEINDE_CODE','avg_travel_min'] aligned 1:1 with the municipalities.\n",
    "    \"\"\"\n",
    "    assert {\"GEMEINDE_CODE\",\"geometry\"} <= set(g_polys.columns), \\\n",
    "        \"g_polys must have ['GEMEINDE_CODE','geometry']\"\n",
    "\n",
    "    g_polys = _ensure_lv95(g_polys)\n",
    "\n",
    "    seed_ids = match_station_ids_by_name(origin_name)\n",
    "    if not seed_ids:\n",
    "        raise ValueError(f\"No station found for '{origin_name}'\")\n",
    "\n",
    "    # 1) Station-level shortest paths\n",
    "    dist_df = dijkstra_from_sources(seed_ids)\n",
    "    st_pts  = stations_pts.merge(dist_df, on=\"station_id\", how=\"left\")  # requires global stations_pts\n",
    "    valid   = st_pts.dropna(subset=[\"time_sec_station\"]).copy()\n",
    "    if valid.empty:\n",
    "        # keep output aligned to g_polys\n",
    "        return g_polys[[\"GEMEINDE_CODE\"]].assign(avg_travel_min=np.nan)\n",
    "\n",
    "    # 2) In-polygon average (rail minutes)\n",
    "    st_in = gpd.sjoin(\n",
    "        valid[[\"station_id\",\"time_sec_station\",\"geometry\"]],\n",
    "        g_polys[[\"GEMEINDE_CODE\",\"geometry\"]],\n",
    "        how=\"left\", predicate=\"within\"\n",
    "    ).dropna(subset=[\"GEMEINDE_CODE\"])\n",
    "    inpoly = (st_in.groupby(\"GEMEINDE_CODE\", as_index=False)[\"time_sec_station\"].mean())\n",
    "    inpoly[\"avg_travel_min\"] = inpoly[\"time_sec_station\"] / 60.0\n",
    "    inpoly = inpoly[[\"GEMEINDE_CODE\",\"avg_travel_min\"]].copy()\n",
    "\n",
    "    # 3) Soft-min via K nearest stations (rail + walk)\n",
    "    g_cent = g_polys.copy()\n",
    "    g_cent[\"centroid\"] = g_cent.geometry.representative_point()\n",
    "    g_cent = g_cent.set_geometry(\"centroid\")\n",
    "\n",
    "    # Build K-nearest efficiently with multiple sjoin_nearest passes (left length preserved)\n",
    "    remaining = g_cent.copy()\n",
    "    cand = valid[[\"station_id\",\"time_sec_station\",\"geometry\"]].copy()\n",
    "    nearest = []\n",
    "    for k in range(K_NEAREST_ST):\n",
    "        kn = gpd.sjoin_nearest(remaining, cand, how=\"left\", distance_col=f\"dist_m_{k}\")\n",
    "        # Normalize cols regardless of *_left/*_right suffixes\n",
    "        if \"GEMEINDE_CODE_left\" in kn.columns:\n",
    "            kn = kn.rename(columns={\"GEMEINDE_CODE_left\":\"GEMEINDE_CODE\"})\n",
    "        if \"time_sec_station_right\" in kn.columns:\n",
    "            kn = kn.rename(columns={\"time_sec_station_right\": f\"time_sec_{k}\"})\n",
    "        elif \"time_sec_station\" in kn.columns:\n",
    "            kn = kn.rename(columns={\"time_sec_station\": f\"time_sec_{k}\"})\n",
    "        if \"station_id_right\" in kn.columns:\n",
    "            kn = kn.rename(columns={\"station_id_right\": f\"sid_{k}\"})\n",
    "        elif \"station_id\" in kn.columns:\n",
    "            kn = kn.rename(columns={\"station_id\": f\"sid_{k}\"})\n",
    "        nearest.append(kn[[\"GEMEINDE_CODE\", f\"sid_{k}\", f\"time_sec_{k}\", f\"dist_m_{k}\"]].copy())\n",
    "\n",
    "        # exclude matched to diversify\n",
    "        matched = kn.get(f\"sid_{k}\", pd.Series(dtype=object)).dropna().astype(str).unique().tolist()\n",
    "        if matched:\n",
    "            cand = cand[~cand[\"station_id\"].astype(str).isin(matched)].copy()\n",
    "\n",
    "    knn = nearest[0]\n",
    "    for tdf in nearest[1:]:\n",
    "        knn = knn.merge(tdf, on=\"GEMEINDE_CODE\", how=\"left\")\n",
    "\n",
    "    tot_cols=[]\n",
    "    for k in range(K_NEAREST_ST):\n",
    "        dcol, tcol = f\"dist_m_{k}\", f\"time_sec_{k}\"\n",
    "        if dcol in knn.columns and tcol in knn.columns:\n",
    "            knn[dcol] = np.minimum(knn[dcol].fillna(np.inf), MAX_WALK_M)\n",
    "            walk_min  = knn[dcol] / WALK_M_PER_MIN\n",
    "            rail_min  = knn[tcol] / 60.0\n",
    "            knn[f\"total_min_{k}\"] = rail_min + walk_min\n",
    "            tot_cols.append(f\"total_min_{k}\")\n",
    "\n",
    "    if tot_cols:\n",
    "        arr = knn[tot_cols].to_numpy(dtype=\"float64\")                      # shape: [n_gemeinde, K]\n",
    "        arr = np.where(np.isfinite(arr), arr, np.inf)\n",
    "        Z   = np.exp(-np.clip(arr, 0, np.inf) / SOFTMIN_TAU_MIN)\n",
    "        Z[arr == np.inf] = 0.0\n",
    "        soft = -SOFTMIN_TAU_MIN * np.log(np.clip(Z.sum(axis=1), 1e-9, np.inf))\n",
    "        knn_out = pd.DataFrame({\"GEMEINDE_CODE\": knn[\"GEMEINDE_CODE\"].values, \"softmin_min\": soft})\n",
    "    else:\n",
    "        knn_out = pd.DataFrame({\"GEMEINDE_CODE\": knn[\"GEMEINDE_CODE\"].values, \"softmin_min\": np.nan})\n",
    "\n",
    "    # 4) Combine in-polygon & soft-min (rail+walk)\n",
    "    comb = g_polys[[\"GEMEINDE_CODE\",\"geometry\"]].merge(inpoly, on=\"GEMEINDE_CODE\", how=\"left\")\n",
    "    comb = comb.merge(knn_out, on=\"GEMEINDE_CODE\", how=\"left\")\n",
    "    comb[\"avg_travel_min\"] = comb[\"avg_travel_min\"].fillna(comb[\"softmin_min\"]).astype(float)\n",
    "    comb = comb.drop(columns=[\"softmin_min\"])\n",
    "\n",
    "    # 5) Physics floor and origin=0 rule\n",
    "    seeds_geom = stations_pts.loc[stations_pts[\"station_id\"].isin(seed_ids), \"geometry\"]\n",
    "    if len(seeds_geom) == 0:\n",
    "        seeds_geom = stations_pts.iloc[:1][\"geometry\"]\n",
    "    origin_pt = _union_all_geoms(seeds_geom).centroid\n",
    "\n",
    "    rep_pts = comb[\"geometry\"].representative_point()\n",
    "    dist_m  = rep_pts.distance(origin_pt).to_numpy(dtype=\"float64\")\n",
    "    floor_min = BOARD_MIN + (dist_m / 1000.0) / VMAX_GLOBAL_KMH * 60.0\n",
    "    comb[\"avg_travel_min\"] = np.maximum(comb[\"avg_travel_min\"].to_numpy(dtype=\"float64\"), floor_min)\n",
    "\n",
    "    origin_code = g_polys.loc[g_polys.contains(origin_pt), \"GEMEINDE_CODE\"]\n",
    "    if not origin_code.empty:\n",
    "        comb.loc[comb[\"GEMEINDE_CODE\"] == origin_code.iloc[0], \"avg_travel_min\"] = 0.0\n",
    "\n",
    "    # 6) Length-safe Gaussian smoothing across Gemeinden\n",
    "    # Build arrays aligned to base rows (no joins that could duplicate)\n",
    "    base = g_polys.copy()\n",
    "    base[\"centroid\"] = base.geometry.representative_point()\n",
    "    base = base.set_geometry(\"centroid\")\n",
    "\n",
    "    src = base.merge(comb[[\"GEMEINDE_CODE\",\"avg_travel_min\"]], on=\"GEMEINDE_CODE\", how=\"inner\")\n",
    "    src = src.dropna(subset=[\"avg_travel_min\"])\n",
    "\n",
    "    if len(src) == 0:\n",
    "        return comb[[\"GEMEINDE_CODE\",\"avg_travel_min\"]]\n",
    "\n",
    "    # Coordinates\n",
    "    bx = base.geometry.x.to_numpy(dtype=\"float64\")\n",
    "    by = base.geometry.y.to_numpy(dtype=\"float64\")\n",
    "    sx = src.geometry.x.to_numpy(dtype=\"float64\")\n",
    "    sy = src.geometry.y.to_numpy(dtype=\"float64\")\n",
    "    st = src[\"avg_travel_min\"].to_numpy(dtype=\"float64\")\n",
    "\n",
    "    # Distance matrix [n_base, n_src]\n",
    "    dx = bx[:, None] - sx[None, :]\n",
    "    dy = by[:, None] - sy[None, :]\n",
    "    D  = np.hypot(dx, dy)\n",
    "\n",
    "    # Gaussian weights and weighted mean per base row\n",
    "    W  = np.exp(-0.5 * (D / SMOOTH_SIGMA_M)**2)\n",
    "    num = W @ st\n",
    "    den = W.sum(axis=1)\n",
    "    smoothed = np.where(den > 0, num / den, np.nan)\n",
    "\n",
    "    out = base[[\"GEMEINDE_CODE\"]].copy()\n",
    "    out[\"avg_travel_min\"] = smoothed\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1671,
     "status": "ok",
     "timestamp": 1759138503967,
     "user": {
      "displayName": "Ingo Stallknecht",
      "userId": "04299073505858791564"
     },
     "user_tz": -120
    },
    "id": "TGiaQTe0eVKh",
    "outputId": "c52360ce-99af-4887-ed2d-4de04020013c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Travel-time coverage: 2116 / 2116\n"
     ]
    }
   ],
   "source": [
    "# ===== 7) Compute commute from chosen origin (fixed end-to-end) =====\n",
    "ORIGIN_STATION = \"Zürich HB\"  # change for exploration\n",
    "\n",
    "# Work in LV95 for spatial ops and pass only required columns\n",
    "g_m = g.to_crs(CRS_CH)[[\"GEMEINDE_CODE\",\"geometry\"]].copy()\n",
    "\n",
    "# Compute minutes per Gemeinde from the selected origin\n",
    "tt_from_origin = compute_commute_minutes_from(ORIGIN_STATION, g_m)\n",
    "\n",
    "# Merge back (ensure keys are comparable strings)\n",
    "tt_from_origin[\"GEMEINDE_CODE\"] = tt_from_origin[\"GEMEINDE_CODE\"].astype(str)\n",
    "g[\"GEMEINDE_CODE\"] = g[\"GEMEINDE_CODE\"].astype(str)\n",
    "\n",
    "# Replace old 'avg_travel_min' with the new one for this origin\n",
    "g = g.drop(columns=[c for c in [\"avg_travel_min\"] if c in g.columns])\n",
    "g = g.merge(tt_from_origin, on=\"GEMEINDE_CODE\", how=\"left\", validate=\"many_to_one\")\n",
    "\n",
    "g_tt_plot = g.dropna(subset=[\"avg_travel_min\"]).copy()\n",
    "print(\"Travel-time coverage:\", len(g_tt_plot), \"/\", len(g))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1759138504007,
     "user": {
      "displayName": "Ingo Stallknecht",
      "userId": "04299073505858791564"
     },
     "user_tz": -120
    },
    "id": "x7bQJ2ntectu",
    "outputId": "693c2162-0455-42ab-f9c1-a07a34126202"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preference score coverage: 0.995\n"
     ]
    }
   ],
   "source": [
    "# ===== 8) Preference scoring (canonical baseline + logistic from scenarios) =====\n",
    "# Canonical score uses intercept + exponential penalty\n",
    "CANON_A = 2.0     # housing weight\n",
    "CANON_B = 2.0     # commute penalty weight\n",
    "CANON_K = 3.0     # curvature strength\n",
    "\n",
    "v_min, v_max = robust_range(g[\"vacancy_pct\"])\n",
    "t_min, t_max = robust_range(g[\"avg_travel_min\"])\n",
    "\n",
    "mask = g[\"vacancy_pct\"].notna() & g[\"avg_travel_min\"].notna()\n",
    "g_pref = g.loc[mask, [\"GEMEINDE_CODE\",\"vacancy_pct\",\"avg_travel_min\"]].copy()\n",
    "\n",
    "if len(g_pref):\n",
    "    v_all = norm01(pd.to_numeric(g_pref[\"vacancy_pct\"], errors=\"coerce\").values, v_min, v_max)\n",
    "    t_all = norm01(pd.to_numeric(g_pref[\"avg_travel_min\"], errors=\"coerce\").values, t_min, t_max)\n",
    "    pen_t = exp_penalty_monotone(t_all, CANON_K)\n",
    "    util0 = CANON_A * v_all - CANON_B * pen_t\n",
    "    w0 = -np.median(util0)  # center near 50\n",
    "    score = 100.0 * (1.0 / (1.0 + np.exp(-(w0 + util0))))\n",
    "    g_pref[\"preference_score\"] = score\n",
    "    g = g.merge(g_pref[[\"GEMEINDE_CODE\",\"preference_score\"]], on=\"GEMEINDE_CODE\", how=\"left\")\n",
    "else:\n",
    "    g[\"preference_score\"] = np.nan\n",
    "    w0 = 0.0\n",
    "\n",
    "print(\"Preference score coverage:\", g[\"preference_score\"].notna().mean().round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 6158,
     "status": "ok",
     "timestamp": 1759138510168,
     "user": {
      "displayName": "Ingo Stallknecht",
      "userId": "04299073505858791564"
     },
     "user_tz": -120
    },
    "id": "ZFUuGXTXfQR1"
   },
   "outputs": [],
   "source": [
    "# ===== 9) (Optional) Static maps for notebook report =====\n",
    "try:\n",
    "    # Vacancy\n",
    "    g_plot = g.dropna(subset=[\"vacancy_pct\"]).copy()\n",
    "    if len(g_plot):\n",
    "        fig, ax = plt.subplots(figsize=(10,10), dpi=150)\n",
    "        g_plot.plot(column=\"vacancy_pct\", ax=ax, cmap=\"RdYlGn\", edgecolor=\"white\", linewidth=0.15,\n",
    "                    legend=True, legend_kwds={\"label\":\"Vacancy rate (%)\",\"shrink\":0.6})\n",
    "        ax.set_axis_off(); ax.set_title(\"Vacancy rate\", pad=10)\n",
    "        if HAS_SCALEBAR: ax.add_artist(ScaleBar(dx=1, units=\"m\", dimension=\"si-length\", location=\"lower left\", box_alpha=0.8))\n",
    "        _savefig(os.path.join(OUT_DIR,\"vacancy_gemeinden.png\"))\n",
    "\n",
    "    # Commute\n",
    "    g_tt_plot = g.dropna(subset=[\"avg_travel_min\"]).copy()\n",
    "    if len(g_tt_plot):\n",
    "        fig, ax = plt.subplots(figsize=(10,10), dpi=150)\n",
    "        g_tt_plot.plot(column=\"avg_travel_min\", ax=ax, cmap=\"RdYlGn_r\", edgecolor=\"white\", linewidth=0.15,\n",
    "                       legend=True, legend_kwds={\"label\":\"Commute time (min)\",\"shrink\":0.6})\n",
    "        ax.set_axis_off(); ax.set_title(\"Commute time from origin\", pad=10)\n",
    "        if HAS_SCALEBAR: ax.add_artist(ScaleBar(dx=1, units=\"m\", dimension=\"si-length\", location=\"lower left\", box_alpha=0.8))\n",
    "        _savefig(os.path.join(OUT_DIR,\"commute_from_origin.png\"))\n",
    "\n",
    "    # Preference score\n",
    "    if g[\"preference_score\"].notna().any():\n",
    "        g_ps = g.dropna(subset=[\"preference_score\"]).copy()\n",
    "        fig, ax = plt.subplots(figsize=(10,10), dpi=150)\n",
    "        g_ps.plot(column=\"preference_score\", ax=ax, cmap=\"RdYlGn\", edgecolor=\"white\", linewidth=0.15,\n",
    "                  legend=True, vmin=0, vmax=100, legend_kwds={\"label\":\"Preference score (0–100)\",\"shrink\":0.6})\n",
    "        ax.set_axis_off(); ax.set_title(\"Preference score\", pad=10)\n",
    "        if HAS_SCALEBAR: ax.add_artist(ScaleBar(dx=1, units=\"m\", dimension=\"si-length\", location=\"lower left\", box_alpha=0.8))\n",
    "        _savefig(os.path.join(OUT_DIR,\"preference_score.png\"))\n",
    "    else:\n",
    "        print(\"ℹ️ Preference score not available — skipping plot.\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Skipping static maps: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7512,
     "status": "ok",
     "timestamp": 1759138517688,
     "user": {
      "displayName": "Ingo Stallknecht",
      "userId": "04299073505858791564"
     },
     "user_tz": -120
    },
    "id": "I4iEav6XehTU",
    "outputId": "dc68af33-8011-41a4-9346-54f51a656743"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported to: /content/artifacts\n",
      "Files: ['gemeinden_centroids.parquet', 'stations_lookup.csv', 'meta.json', 'gemeinden_simplified.geojson', 'gemeinden.csv', 'gemeinden.geojson']\n"
     ]
    }
   ],
   "source": [
    "# ===== 10) Export artifacts (full + simplified + centroids + meta) =====\n",
    "# Columns the app expects\n",
    "export_cols = [\"GEMEINDE_CODE\",\"GEMEINDE_NAME\",\"vacancy_pct\",\"avg_travel_min\",\"preference_score\",\"geometry\"]\n",
    "\n",
    "g_export = g.copy()\n",
    "if g_export.crs is None:\n",
    "    try: g_export = g_export.set_crs(CRS_CH)\n",
    "    except Exception: pass\n",
    "try:\n",
    "    g_export = g_export.to_crs(CRS_WGS84)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Ensure required cols exist\n",
    "for c in export_cols:\n",
    "    if c not in g_export.columns and c != \"geometry\":\n",
    "        g_export[c] = np.nan\n",
    "g_export = g_export[export_cols]\n",
    "\n",
    "# Full GeoJSON\n",
    "gj_full = os.path.join(ARTIFACTS, \"gemeinden.geojson\")\n",
    "g_export.to_file(gj_full, driver=\"GeoJSON\")\n",
    "\n",
    "# Simplified polygons (for faster app)\n",
    "g_slim = g_export.copy()\n",
    "g_slim[\"geometry\"] = g_slim.geometry.simplify(0.0012, preserve_topology=True)  # ~120–200m\n",
    "gj_slim = os.path.join(ARTIFACTS, \"gemeinden_simplified.geojson\")\n",
    "g_slim.to_file(gj_slim, driver=\"GeoJSON\")\n",
    "\n",
    "# Centroids parquet (fast PyDeck mode)\n",
    "cent = g_export.copy()\n",
    "cent[\"geometry\"] = cent.geometry.representative_point()\n",
    "cent_pq = os.path.join(ARTIFACTS, \"gemeinden_centroids.parquet\")\n",
    "cent.to_parquet(cent_pq, index=False)\n",
    "\n",
    "# CSV (no geometry)\n",
    "g_export.drop(columns=[\"geometry\"]).to_csv(os.path.join(ARTIFACTS, \"gemeinden.csv\"), index=False)\n",
    "\n",
    "# Stations lookup\n",
    "station_choices = (stops[[\"station_id\",\"stop_name\"]].dropna().drop_duplicates().sort_values(\"stop_name\"))\n",
    "station_choices = station_choices[~station_choices[\"stop_name\"].str.contains(\"Bahn-2000\", case=False, na=False)]\n",
    "station_choices.to_csv(os.path.join(ARTIFACTS, \"stations_lookup.csv\"), index=False)\n",
    "\n",
    "# Meta\n",
    "meta = {\n",
    "    \"origin_station\": ORIGIN_STATION,\n",
    "    \"canonical_weights\": {\"a\": float(CANON_A), \"b\": float(CANON_B), \"w0\": float(w0)},\n",
    "    \"penalty_k\": float(CANON_K),\n",
    "    \"v_min\": float(v_min), \"v_max\": float(v_max),\n",
    "    \"t_min\": float(t_min), \"t_max\": float(t_max)\n",
    "}\n",
    "with open(os.path.join(ARTIFACTS, \"meta.json\"), \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"✅ Exported to:\", ARTIFACTS)\n",
    "print(\"Files:\", os.listdir(ARTIFACTS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45476,
     "status": "ok",
     "timestamp": 1759138563172,
     "user": {
      "displayName": "Ingo Stallknecht",
      "userId": "04299073505858791564"
     },
     "user_tz": -120
    },
    "id": "4HcABNWQfw_n",
    "outputId": "f4c3c533-2365-4acb-f88c-4671ff4ccfa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origins: Aarau, Bahn-2000-Strecke, Basel SBB, Bellinzona, Bern, Brig, Chur, Fribourg/Freiburg, Genève, Lausanne, Liestal, Lugano, Luzern, Morges, Neuchâtel, Olten, Renens VD, Sion, Spiez, St. Gallen, Thun, Winterthur, Zug, Zürich Flughafen, Zürich HB, Zürich Oerlikon\n",
      "✅ Wrote /content/artifacts/tt_by_origin.parquet (55,016 rows, 26 origins).\n"
     ]
    }
   ],
   "source": [
    "# ===== 10′b) Multi-origin export (top-20 busiest + 6 extras, origin=0 rule) =====\n",
    "# Writes: tt_by_origin.parquet with [GEMEINDE_CODE, avg_travel_min, origin_name]\n",
    "import math\n",
    "\n",
    "# Build name map once\n",
    "name_map = (stops.groupby(\"station_id\")[\"stop_name\"].agg(lambda s: s.value_counts().idxmax()))\n",
    "\n",
    "def match_station_ids_by_name_exactish(nm: str) -> List[str]:\n",
    "    n0 = _normtxt(nm)\n",
    "    exact = [sid for sid, n in name_map.items() if isinstance(n, str) and _normtxt(n)==n0]\n",
    "    if exact: return [str(exact[0])]\n",
    "    starts = [sid for sid, n in name_map.items() if isinstance(n, str) and _normtxt(n).startswith(n0)]\n",
    "    if starts: return [str(starts[0])]\n",
    "    contains = [sid for sid, n in name_map.items() if isinstance(n, str) and n0 in _normtxt(n)]\n",
    "    return [str(contains[0])] if contains else []\n",
    "\n",
    "# Busiest stations\n",
    "by_station = (st_times.groupby(\"station_id\", as_index=False)[\"stop_id\"]\n",
    "                      .count().rename(columns={\"stop_id\":\"events\"})\n",
    "                      .sort_values(\"events\", ascending=False))\n",
    "top20 = by_station.head(20).assign(origin_station_id=lambda d: d[\"station_id\"].astype(str))\n",
    "top20[\"origin_name\"] = top20[\"origin_station_id\"].map(name_map.to_dict())\n",
    "\n",
    "# + 6 curated extras\n",
    "EXTRA_ORIGINS = [\"Neuchâtel\",\"Fribourg/Freiburg\",\"Sion\",\"Lugano\",\"Bellinzona\",\"Chur\"]\n",
    "extra_rows=[]\n",
    "for nm in EXTRA_ORIGINS:\n",
    "    ids = match_station_ids_by_name_exactish(nm)\n",
    "    if not ids:\n",
    "        print(f\"ℹ️ Extra origin '{nm}' not found; skip.\")\n",
    "        continue\n",
    "    sid=ids[0]\n",
    "    if sid in set(top20[\"origin_station_id\"].astype(str)):\n",
    "        continue\n",
    "    extra_rows.append({\"origin_station_id\": sid, \"origin_name\": name_map.get(sid, nm)})\n",
    "if extra_rows:\n",
    "    top20 = pd.concat([top20[[\"origin_station_id\",\"origin_name\"]], pd.DataFrame(extra_rows)], ignore_index=True)\n",
    "\n",
    "top20 = top20.dropna(subset=[\"origin_name\"]).drop_duplicates(\"origin_station_id\").reset_index(drop=True)\n",
    "origin_names = sorted({str(x) for x in top20[\"origin_name\"].tolist()}, key=str.casefold)\n",
    "print(\"Origins:\", \", \".join(origin_names))\n",
    "\n",
    "# Compute per origin\n",
    "rows=[]\n",
    "for _, r in top20.iterrows():\n",
    "    origin_name = str(r[\"origin_name\"])\n",
    "    res = compute_commute_minutes_from(origin_name, g_m[[\"GEMEINDE_CODE\",\"geometry\"]].copy())\n",
    "    res[\"origin_name\"] = origin_name\n",
    "    rows.append(res[[\"GEMEINDE_CODE\",\"avg_travel_min\",\"origin_name\"]])\n",
    "\n",
    "tt_by_origin = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=[\"GEMEINDE_CODE\",\"avg_travel_min\",\"origin_name\"])\n",
    "tt_by_origin[\"avg_travel_min\"] = pd.to_numeric(tt_by_origin[\"avg_travel_min\"], errors=\"coerce\")\n",
    "\n",
    "outp = os.path.join(ARTIFACTS, \"tt_by_origin.parquet\")\n",
    "tt_by_origin.to_parquet(outp, index=False)\n",
    "print(f\"✅ Wrote {outp} ({len(tt_by_origin):,} rows, {tt_by_origin['origin_name'].nunique()} origins).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1708,
     "status": "ok",
     "timestamp": 1759138564885,
     "user": {
      "displayName": "Ingo Stallknecht",
      "userId": "04299073505858791564"
     },
     "user_tz": -120
    },
    "id": "lqWPn0P8PFKR",
    "outputId": "2fc30631-ed75-4293-8b93-8f099da48769"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Artifacts & report OK.\n"
     ]
    }
   ],
   "source": [
    "# ===== 11) Lightweight QA checks & summary report =====\n",
    "def _assert_exists(p):\n",
    "    assert os.path.exists(p), f\"Missing file: {p}\"\n",
    "\n",
    "for f in [\"gemeinden.geojson\",\"gemeinden_simplified.geojson\",\"gemeinden_centroids.parquet\",\"gemeinden.csv\",\"stations_lookup.csv\",\"meta.json\"]:\n",
    "    _assert_exists(os.path.join(ARTIFACTS, f))\n",
    "\n",
    "gj = gpd.read_file(os.path.join(ARTIFACTS, \"gemeinden.geojson\"))\n",
    "needed = [\"GEMEINDE_CODE\",\"GEMEINDE_NAME\",\"vacancy_pct\",\"avg_travel_min\",\"preference_score\",\"geometry\"]\n",
    "for c in needed:\n",
    "    assert c in gj.columns, f\"Column missing in geojson: {c}\"\n",
    "\n",
    "report = {\n",
    "    \"n_gemeinden\": int(len(g)),\n",
    "    \"vacancy_coverage_pct\": float(100 * g[\"vacancy_pct\"].notna().mean()),\n",
    "    \"commute_coverage_pct\": float(100 * g[\"avg_travel_min\"].notna().mean()),\n",
    "    \"preference_coverage_pct\": float(100 * g[\"preference_score\"].notna().mean()),\n",
    "    \"vacancy_summary\": g[\"vacancy_pct\"].describe().to_dict(),\n",
    "    \"commute_summary\": g[\"avg_travel_min\"].describe().to_dict(),\n",
    "}\n",
    "with open(os.path.join(ARTIFACTS, \"report.json\"), \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "print(\"✅ Artifacts & report OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1759138564891,
     "user": {
      "displayName": "Ingo Stallknecht",
      "userId": "04299073505858791564"
     },
     "user_tz": -120
    },
    "id": "cua9pt4Jbzxx"
   },
   "outputs": [],
   "source": [
    "ORIGIN_FOR_EXAMPLE = \"Zürich HB\"\n",
    "\n",
    "# If many NaNs in avg_travel_min, try to recompute from Zürich HB (only if helper is available)\n",
    "try:\n",
    "    need_tt = (\"avg_travel_min\" not in g.columns) or (g[\"avg_travel_min\"].isna().mean() > 0.1)\n",
    "    if need_tt and \"compute_commute_minutes_from\" in globals():\n",
    "        # Minimal LV95 frame for spatial join\n",
    "        g_m = g.to_crs(CRS_CH)[[\"GEMEINDE_CODE\",\"geometry\"]].copy()\n",
    "        tt_from_origin = compute_commute_minutes_from(ORIGIN_FOR_EXAMPLE, g_m)\n",
    "        tt_from_origin[\"GEMEINDE_CODE\"] = tt_from_origin[\"GEMEINDE_CODE\"].astype(str)\n",
    "        g[\"GEMEINDE_CODE\"] = g[\"GEMEINDE_CODE\"].astype(str)\n",
    "        g = g.drop(columns=[c for c in [\"avg_travel_min\"] if c in g.columns])\n",
    "        g = g.merge(tt_from_origin, on=\"GEMEINDE_CODE\", how=\"left\", validate=\"many_to_one\")\n",
    "        print(f\"Recomputed commute times from '{ORIGIN_FOR_EXAMPLE}'. Coverage:\",\n",
    "              g[\"avg_travel_min\"].notna().mean())\n",
    "except Exception as e:\n",
    "    print(\"Skipping recompute (helper not available or failed):\", e)\n",
    "\n",
    "# If preference_score missing, compute a canonical one using the same formula as Step 10\n",
    "if \"preference_score\" not in g.columns or g[\"preference_score\"].isna().all():\n",
    "    import numpy as np, pandas as pd\n",
    "    def norm01(x, lo, hi):\n",
    "        x = np.asarray(pd.to_numeric(x, errors=\"coerce\"), dtype=\"float64\")\n",
    "        return (x - lo) / (hi - lo + 1e-9)\n",
    "    def exp_penalty_monotone(t_norm, k):\n",
    "        t_norm = np.clip(np.asarray(t_norm, dtype=\"float64\"), 0.0, 1.0)\n",
    "        if k <= 1e-6: return t_norm\n",
    "        return (1.0 - np.exp(-k * t_norm)) / (1.0 - np.exp(-k))\n",
    "\n",
    "    # Use the same hyperparams exported in Step 10 (fallbacks if not present)\n",
    "    CANON_A = 2.0\n",
    "    CANON_B = 2.0\n",
    "    CANON_K = 3.0\n",
    "\n",
    "    v_min = float(np.nanmin(pd.to_numeric(g[\"vacancy_pct\"], errors=\"coerce\")))\n",
    "    v_max = float(np.nanmax(pd.to_numeric(g[\"vacancy_pct\"], errors=\"coerce\")))\n",
    "    t_min = float(np.nanmin(pd.to_numeric(g[\"avg_travel_min\"], errors=\"coerce\")))\n",
    "    t_max = float(np.nanmax(pd.to_numeric(g[\"avg_travel_min\"], errors=\"coerce\")))\n",
    "\n",
    "    mask = g[\"vacancy_pct\"].notna() & g[\"avg_travel_min\"].notna()\n",
    "    v_all = norm01(g.loc[mask, \"vacancy_pct\"],    v_min, v_max)\n",
    "    t_all = norm01(g.loc[mask, \"avg_travel_min\"], t_min, t_max)\n",
    "    util0 = CANON_A * v_all - CANON_B * exp_penalty_monotone(t_all, CANON_K)\n",
    "\n",
    "    # Center around 50\n",
    "    w0 = -np.median(util0)\n",
    "    score = 100.0 * (1.0 / (1.0 + np.exp(-(w0 + util0))))\n",
    "    g.loc[mask, \"preference_score\"] = np.clip(score, 0, 100)\n",
    "\n",
    "    print(\"Computed canonical preference_score for example plots.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "output_embedded_package_id": "1VU44szDgozhznoScdRhCqlOdR--80z66"
    },
    "executionInfo": {
     "elapsed": 8004,
     "status": "ok",
     "timestamp": 1759138572898,
     "user": {
      "displayName": "Ingo Stallknecht",
      "userId": "04299073505858791564"
     },
     "user_tz": -120
    },
    "id": "u-kY3_1bb4BL",
    "outputId": "cae7067d-1856-4b08-c519-74ae155d94b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Notebook Folium maps (origin=0 + zero-shift) =====\n",
    "# Place this AFTER your export step (section 10), so artifacts exist.\n",
    "\n",
    "import os, json, numpy as np, pandas as pd, geopandas as gpd\n",
    "import folium, branca.colormap as bcm\n",
    "from pathlib import Path\n",
    "\n",
    "ARTIFACTS = Path(\"/content/artifacts\")\n",
    "GEOJSON   = ARTIFACTS / \"gemeinden.geojson\"\n",
    "META_JSON = ARTIFACTS / \"meta.json\"\n",
    "TTO_PARQ  = ARTIFACTS / \"tt_by_origin.parquet\"   # optional (multi-origin)\n",
    "\n",
    "assert GEOJSON.exists() and META_JSON.exists(), \"Run the export step first (section 10).\"\n",
    "\n",
    "# --- Load data ---\n",
    "g = gpd.read_file(GEOJSON)\n",
    "if g.crs is None or str(g.crs).lower() != \"epsg:4326\":\n",
    "    g = g.to_crs(4326)\n",
    "\n",
    "meta = json.loads(META_JSON.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "tto = None\n",
    "if TTO_PARQ.exists():\n",
    "    tto = pd.read_parquet(TTO_PARQ)\n",
    "    tto[\"GEMEINDE_CODE\"]  = tto[\"GEMEINDE_CODE\"].astype(str)\n",
    "    tto[\"avg_travel_min\"] = pd.to_numeric(tto[\"avg_travel_min\"], errors=\"coerce\")\n",
    "\n",
    "# --- Helpers ---\n",
    "def robust_range(series, lo_q=1, hi_q=99):\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    s = s[np.isfinite(s)]\n",
    "    if s.empty: return (0.0, 1.0)\n",
    "    lo = float(np.nanpercentile(s, lo_q))\n",
    "    hi = float(np.nanpercentile(s, hi_q))\n",
    "    if not np.isfinite(lo) or not np.isfinite(hi) or hi <= lo:\n",
    "        lo, hi = float(np.nanmin(s)), float(np.nanmax(s))\n",
    "    if not np.isfinite(lo) or not np.isfinite(hi) or hi <= lo:\n",
    "        lo, hi = 0.0, 1.0\n",
    "    return lo, hi\n",
    "\n",
    "def norm01(x, lo, hi):\n",
    "    x = np.asarray(x, dtype=\"float64\")\n",
    "    if hi <= lo + 1e-9: return np.zeros_like(x)\n",
    "    return (np.clip(x, lo, hi) - lo) / (hi - lo)\n",
    "\n",
    "def commute_penalty_shape(t_norm, k):\n",
    "    # Smooth, monotone penalty; larger k ≈ stronger curvature\n",
    "    t = np.clip(np.asarray(t_norm, dtype=\"float64\"), 0.0, 1.0)\n",
    "    kk = max(0.2, float(k))\n",
    "    return 1.0 - np.power(1.0 - t, kk)\n",
    "\n",
    "def enforce_origin_zero_and_shift(df_wgs84: gpd.GeoDataFrame,\n",
    "                                  tt_sel: pd.DataFrame | None) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    If per-origin times are provided, merge them and:\n",
    "      1) force the min-time municipality(ies) to 0.0,\n",
    "      2) subtract the (per-origin) global min from all effective times so tooltips show minutes from origin.\n",
    "    If not provided, zero-shift the existing avg_travel_min in df.\n",
    "    \"\"\"\n",
    "    df = df_wgs84.copy()\n",
    "    df[\"GEMEINDE_CODE\"] = df[\"GEMEINDE_CODE\"].astype(str)\n",
    "\n",
    "    if tt_sel is not None and not tt_sel.empty:\n",
    "        tmp = tt_sel[[\"GEMEINDE_CODE\",\"avg_travel_min\"]].copy()\n",
    "        tmp = tmp.rename(columns={\"avg_travel_min\": \"avg_travel_min_origin\"})\n",
    "        tmp[\"GEMEINDE_CODE\"] = tmp[\"GEMEINDE_CODE\"].astype(str)\n",
    "        df = df.merge(tmp, on=\"GEMEINDE_CODE\", how=\"left\")\n",
    "        df[\"avg_travel_min_eff\"] = df[\"avg_travel_min_origin\"].combine_first(df.get(\"avg_travel_min\"))\n",
    "    else:\n",
    "        df[\"avg_travel_min_eff\"] = df.get(\"avg_travel_min\")\n",
    "\n",
    "    # Zero-shift by the minimum and clamp at 0\n",
    "    cur = pd.to_numeric(df[\"avg_travel_min_eff\"], errors=\"coerce\")\n",
    "    if cur.notna().any():\n",
    "        mmin = float(np.nanmin(cur))\n",
    "        # set the municipality(ies) achieving the min to exactly 0.0\n",
    "        min_mask = (cur == mmin)\n",
    "        df.loc[min_mask, \"avg_travel_min_eff\"] = 0.0\n",
    "        # subtract min for everyone to show minutes-from-origin in tooltips\n",
    "        df.loc[~min_mask, \"avg_travel_min_eff\"] = (cur - mmin)[~min_mask]\n",
    "    df[\"avg_travel_min_eff\"] = pd.to_numeric(df[\"avg_travel_min_eff\"], errors=\"coerce\").clip(lower=0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def folium_choropleth(df_map, value_col, legend_label, colors, vmin=None, vmax=None, tooltip_cols=()):\n",
    "    df_map = df_map.copy()\n",
    "    try:\n",
    "        df_map[\"geometry\"] = df_map.geometry.simplify(0.0005, preserve_topology=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    vals = pd.to_numeric(df_map[value_col], errors=\"coerce\")\n",
    "    if vmin is None: vmin = float(np.nanpercentile(vals, 1)) if vals.notna().any() else 0.0\n",
    "    if vmax is None: vmax = float(np.nanpercentile(vals, 99)) if vals.notna().any() else 1.0\n",
    "\n",
    "    cmap = bcm.LinearColormap(colors=colors, vmin=vmin, vmax=vmax); cmap.caption = legend_label\n",
    "    m = folium.Map(location=[46.8, 8.2], zoom_start=7, tiles=\"OpenStreetMap\")\n",
    "\n",
    "    # Friendly tooltips\n",
    "    fields, aliases = [], []\n",
    "    for col, alias in tooltip_cols:\n",
    "        fields.append(col); aliases.append(alias)\n",
    "\n",
    "    def style_fn(feat):\n",
    "        try: val = float(feat[\"properties\"].get(value_col, None))\n",
    "        except Exception: val = None\n",
    "        fill = cmap(val) if val is not None else \"#cccccc\"\n",
    "        return {\"fillColor\": fill, \"color\": \"white\", \"weight\": 0.2, \"fillOpacity\": 0.85}\n",
    "\n",
    "    folium.GeoJson(\n",
    "        data=df_map.to_json(),\n",
    "        style_function=style_fn,\n",
    "        tooltip=folium.features.GeoJsonTooltip(fields=fields, aliases=aliases, localize=True),\n",
    "    ).add_to(m)\n",
    "    cmap.add_to(m)\n",
    "    display(m)\n",
    "\n",
    "# --- Set origin and build effective times (origin=0 + zero-shift) ---\n",
    "ORIGIN_NAME = \"Zürich HB\"\n",
    "\n",
    "if tto is not None and \"origin_name\" in tto.columns and ORIGIN_NAME in tto[\"origin_name\"].astype(str).unique():\n",
    "    tt_sel = tto.loc[tto[\"origin_name\"] == ORIGIN_NAME].copy()\n",
    "else:\n",
    "    tt_sel = None  # fall back to whatever is already in g\n",
    "\n",
    "g_eff = enforce_origin_zero_and_shift(g, tt_sel)\n",
    "\n",
    "# --- Compute preference score (using same mechanics as app) ---\n",
    "cw = meta.get(\"canonical_weights\", {})\n",
    "w0 = float(cw.get(\"w0\", 0.0)); a = float(cw.get(\"a\", 2.0)); b = float(cw.get(\"b\", 2.0))\n",
    "k  = float(meta.get(\"penalty_k\", 1.5))\n",
    "\n",
    "v_lo, v_hi = robust_range(g_eff[\"vacancy_pct\"])\n",
    "t_lo, t_hi = robust_range(g_eff[\"avg_travel_min_eff\"])\n",
    "\n",
    "v_all = norm01(pd.to_numeric(g_eff[\"vacancy_pct\"], errors=\"coerce\").values, v_lo, v_hi)\n",
    "t_all = norm01(pd.to_numeric(g_eff[\"avg_travel_min_eff\"], errors=\"coerce\").values, t_lo, t_hi)\n",
    "pen_t = commute_penalty_shape(t_all, k)\n",
    "util  = (w0 + a * v_all - b * pen_t)\n",
    "\n",
    "finite = np.isfinite(util)\n",
    "if finite.any():\n",
    "    util = util - np.median(util[finite])\n",
    "\n",
    "g_eff[\"preference_score\"] = 100.0 * (1.0 / (1.0 + np.exp(-util)))\n",
    "\n",
    "# --- Render 3 Folium maps ---\n",
    "# 1) Commute only (now origin municipality shows 0.0 in tooltip)\n",
    "g1 = g_eff.dropna(subset=[\"avg_travel_min_eff\"]).copy()\n",
    "g1[\"time_round\"] = pd.to_numeric(g1[\"avg_travel_min_eff\"], errors=\"coerce\").round(1)\n",
    "vmin_c = 0.0\n",
    "vmax_c = float(np.nanpercentile(g1[\"avg_travel_min_eff\"], 99)) if g1[\"avg_travel_min_eff\"].notna().any() else 120.0\n",
    "folium_choropleth(\n",
    "    g1, \"avg_travel_min_eff\",\n",
    "    legend_label=f\"Commute time from {ORIGIN_NAME} (min)\",\n",
    "    colors=[\"green\",\"yellow\",\"red\"],\n",
    "    vmin=vmin_c, vmax=vmax_c,\n",
    "    tooltip_cols=[(\"GEMEINDE_NAME\",\"Municipality\"), (\"time_round\",\"Commute (min)\")]\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "output_embedded_package_id": "1Md0MBozqImjFip0ip7KqSffuD2Du4kEk"
    },
    "executionInfo": {
     "elapsed": 3004,
     "status": "ok",
     "timestamp": 1759138576089,
     "user": {
      "displayName": "Ingo Stallknecht",
      "userId": "04299073505858791564"
     },
     "user_tz": -120
    },
    "id": "eXr_N6JCcQ6N",
    "outputId": "6eb0b1f6-e286-439d-87fe-b2cdefb9a601"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2) Housing only\n",
    "g2 = g_eff.dropna(subset=[\"vacancy_pct\"]).copy()\n",
    "g2[\"vacancy_round\"] = pd.to_numeric(g2[\"vacancy_pct\"], errors=\"coerce\").round(2)\n",
    "folium_choropleth(\n",
    "    g2, \"vacancy_pct\",\n",
    "    legend_label=\"Vacancy rate (%)\",\n",
    "    colors=[\"red\",\"yellow\",\"green\"],\n",
    "    tooltip_cols=[(\"GEMEINDE_NAME\",\"Municipality\"), (\"vacancy_round\",\"Vacancy (%)\")]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1FyTjM6KtfxtoAqRAGa6q4OncAwHw1KIU"
    },
    "executionInfo": {
     "elapsed": 3106,
     "status": "ok",
     "timestamp": 1759138579290,
     "user": {
      "displayName": "Ingo Stallknecht",
      "userId": "04299073505858791564"
     },
     "user_tz": -120
    },
    "id": "y4rTq9XKcYH1",
    "outputId": "f08e4dbd-6d87-4b2f-93bf-8c0229bec059"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3) Preference score\n",
    "g3 = g_eff.dropna(subset=[\"preference_score\"]).copy()\n",
    "g3[\"score_round\"] = pd.to_numeric(g3[\"preference_score\"], errors=\"coerce\").round(1)\n",
    "g3[\"time_round\"]  = pd.to_numeric(g3[\"avg_travel_min_eff\"], errors=\"coerce\").round(1)\n",
    "g3[\"vacancy_round\"] = pd.to_numeric(g3[\"vacancy_pct\"], errors=\"coerce\").round(2)\n",
    "folium_choropleth(\n",
    "    g3, \"preference_score\",\n",
    "    legend_label=\"Preference score (0–100)\",\n",
    "    colors=[\"red\",\"yellow\",\"green\"],\n",
    "    vmin=0.0, vmax=100.0,\n",
    "    tooltip_cols=[(\"GEMEINDE_NAME\",\"Municipality\"),\n",
    "                  (\"score_round\",\"Score\"),\n",
    "                  (\"vacancy_round\",\"Vacancy (%)\"),\n",
    "                  (\"time_round\",\"Commute (min)\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1759138579410,
     "user": {
      "displayName": "Ingo Stallknecht",
      "userId": "04299073505858791564"
     },
     "user_tz": -120
    },
    "id": "wJS_gzL2fy0N",
    "outputId": "e9dc1bbf-af34-4dc0-b1dd-cac866aef4f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved env_info.json\n"
     ]
    }
   ],
   "source": [
    "# ===== 12) Environment metadata (for reproducibility) =====\n",
    "import platform, sys, pkgutil\n",
    "meta_env = {\n",
    "    \"python_version\": sys.version,\n",
    "    \"platform\": platform.platform(),\n",
    "    \"notebook_seed\": SEED,\n",
    "    \"packages\": {m.name: None for m in pkgutil.iter_modules()}  # lightweight presence list\n",
    "}\n",
    "with open(os.path.join(ARTIFACTS, \"env_info.json\"), \"w\") as f:\n",
    "    json.dump(meta_env, f, indent=2)\n",
    "print(\"Saved env_info.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1759138579426,
     "user": {
      "displayName": "Ingo Stallknecht",
      "userId": "04299073505858791564"
     },
     "user_tz": -120
    },
    "id": "z0X0x49tiLJg",
    "outputId": "fce6e5f0-f216-48a5-8f27-bfaa8aa3e65a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote app.py (robust scenarios + origin=0 + zero-shift)\n"
     ]
    }
   ],
   "source": [
    "# --- Overwrite app.py (robust scenarios + origin=0 + zero-shift) ---\n",
    "import os, shutil\n",
    "APP_DIR  = \"/content/streamlit_app\"\n",
    "DATA_DIR = os.path.join(APP_DIR, \"data\")\n",
    "APP_PATH = os.path.join(APP_DIR, \"app.py\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Ensure artifacts exist locally\n",
    "shutil.copyfile(\"/content/artifacts/gemeinden.geojson\", f\"{DATA_DIR}/gemeinden.geojson\")\n",
    "shutil.copyfile(\"/content/artifacts/meta.json\",       f\"{DATA_DIR}/meta.json\")\n",
    "if os.path.exists(\"/content/artifacts/tt_by_origin.parquet\"):\n",
    "    shutil.copyfile(\"/content/artifacts/tt_by_origin.parquet\", f\"{DATA_DIR}/tt_by_origin.parquet\")\n",
    "\n",
    "app_code = r'''\n",
    "import os, json, hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import streamlit as st\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- Setup ----------\n",
    "try:\n",
    "    BASE_DIR = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    BASE_DIR = Path.cwd()\n",
    "DATA_DIR  = BASE_DIR / \"data\"\n",
    "\n",
    "st.set_page_config(page_title=\"Swiss Commute & Housing Explorer\", layout=\"wide\")\n",
    "\n",
    "# ---------- Utils ----------\n",
    "def robust_range(series_like, lo_q=1, hi_q=99):\n",
    "    s = pd.to_numeric(pd.Series(series_like).astype(\"float64\"), errors=\"coerce\")\n",
    "    s = s[np.isfinite(s)]\n",
    "    if s.empty:\n",
    "        return (0.0, 1.0)\n",
    "    lo = float(np.nanpercentile(s, lo_q))\n",
    "    hi = float(np.nanpercentile(s, hi_q))\n",
    "    if not np.isfinite(lo) or not np.isfinite(hi) or hi <= lo:\n",
    "        lo, hi = float(np.nanmin(s)), float(np.nanmax(s))\n",
    "    if not np.isfinite(lo) or not np.isfinite(hi) or hi <= lo:\n",
    "        lo, hi = 0.0, 1.0\n",
    "    return lo, hi\n",
    "\n",
    "def norm01_clipped(x, lo, hi):\n",
    "    x = np.asarray(x, dtype=\"float64\")\n",
    "    lo, hi = float(lo), float(hi)\n",
    "    if hi <= lo + 1e-9:\n",
    "        return np.zeros_like(x)\n",
    "    x = np.clip(x, lo, hi)\n",
    "    return (x - lo) / (hi - lo)\n",
    "\n",
    "def commute_penalty_shape(t_norm, k):\n",
    "    # Smooth monotone penalty (k controls curvature)\n",
    "    t = np.clip(np.asarray(t_norm, dtype=\"float64\"), 0.0, 1.0)\n",
    "    kk = max(0.2, float(k))\n",
    "    return 1.0 - np.power(1.0 - t, kk)\n",
    "\n",
    "def enforce_origin_zero_and_shift(df_wgs84: gpd.GeoDataFrame,\n",
    "                                  tt_sel: pd.DataFrame | None) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    If per-origin times are provided, merge them:\n",
    "      - put into avg_travel_min_eff\n",
    "      - force min municipality(ies) to 0.0\n",
    "      - subtract global min so tooltips show minutes-from-origin\n",
    "    Else, do the same on existing avg_travel_min.\n",
    "    \"\"\"\n",
    "    df = df_wgs84.copy()\n",
    "    df[\"GEMEINDE_CODE\"] = df[\"GEMEINDE_CODE\"].astype(str)\n",
    "\n",
    "    if tt_sel is not None and not tt_sel.empty:\n",
    "        tmp = tt_sel[[\"GEMEINDE_CODE\",\"avg_travel_min\"]].copy()\n",
    "        tmp = tmp.rename(columns={\"avg_travel_min\": \"avg_travel_min_origin\"})\n",
    "        tmp[\"GEMEINDE_CODE\"] = tmp[\"GEMEINDE_CODE\"].astype(str)\n",
    "        df = df.merge(tmp, on=\"GEMEINDE_CODE\", how=\"left\")\n",
    "        df[\"avg_travel_min_eff\"] = df[\"avg_travel_min_origin\"].combine_first(df.get(\"avg_travel_min\"))\n",
    "    else:\n",
    "        df[\"avg_travel_min_eff\"] = df.get(\"avg_travel_min\")\n",
    "\n",
    "    cur = pd.to_numeric(df[\"avg_travel_min_eff\"], errors=\"coerce\")\n",
    "    if cur.notna().any():\n",
    "        mmin = float(np.nanmin(cur))\n",
    "        min_mask = (cur == mmin)\n",
    "        df.loc[min_mask, \"avg_travel_min_eff\"] = 0.0\n",
    "        df.loc[~min_mask, \"avg_travel_min_eff\"] = (cur - mmin)[~min_mask]\n",
    "    df[\"avg_travel_min_eff\"] = pd.to_numeric(df[\"avg_travel_min_eff\"], errors=\"coerce\").clip(lower=0)\n",
    "    return df\n",
    "\n",
    "def resolve_prior(meta, session):\n",
    "    if \"manual_weights\" in session:\n",
    "        mw = session[\"manual_weights\"]\n",
    "        return (float(mw.get(\"w0\", 0.0)),\n",
    "                float(max(0.0, mw.get(\"w_v\", 2.0))),\n",
    "                float(max(0.0, mw.get(\"w_t\", 2.0))))\n",
    "    try:\n",
    "        cw = meta[\"canonical_weights\"]\n",
    "        return (float(cw.get(\"w0\", 0.0)),\n",
    "                float(max(0.0, cw.get(\"a\", 2.0))),\n",
    "                float(max(0.0, cw.get(\"b\", 2.0))))\n",
    "    except Exception:\n",
    "        return (0.0, 2.0, 2.0)\n",
    "\n",
    "def answers_signature(choices_dict):\n",
    "    key_order = sorted(choices_dict.keys())\n",
    "    s = \"|\".join(f\"{k}:{choices_dict[k]}\" for k in key_order)\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def build_anchor_pairs(v_range, t_range, n_each=3, dv_n=0.18, dt_n=0.10):\n",
    "    v_lo, v_hi = v_range; t_lo, t_hi = t_range\n",
    "    v_mid = (v_lo + v_hi) / 2.0\n",
    "    t_mid = (t_lo + t_hi) / 2.0\n",
    "    dv = dv_n * (v_hi - v_lo)\n",
    "    dt = dt_n * (t_hi - t_lo)\n",
    "    rows = []\n",
    "    for _ in range(n_each):\n",
    "        rows.append({\"A_vacancy_pct\": v_mid, \"A_travel_min\": t_mid - dt,\n",
    "                     \"B_vacancy_pct\": v_mid, \"B_travel_min\": t_mid + dt,\n",
    "                     \"choice\": \"A\", \"w\": 0.15})\n",
    "        rows.append({\"A_vacancy_pct\": v_mid + dv, \"A_travel_min\": t_mid,\n",
    "                     \"B_vacancy_pct\": v_mid - dv, \"B_travel_min\": t_mid,\n",
    "                     \"choice\": \"A\", \"w\": 0.15})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def fit_logistic_weighted(sc_df, v_range, t_range, k_ref,\n",
    "                          prior=(0.0,2.0,2.0), l2=0.25,\n",
    "                          clip_w0=(-1.5,1.5), clip_a=(0.0,4.0), clip_b=(0.0,4.0),\n",
    "                          iters=800, lr=0.25):\n",
    "    v_lo, v_hi = v_range; t_lo, t_hi = t_range\n",
    "    vA = sc_df[\"A_vacancy_pct\"].values; vB = sc_df[\"B_vacancy_pct\"].values\n",
    "    tA = sc_df[\"A_travel_min\"].values;  tB = sc_df[\"B_travel_min\"].values\n",
    "    y  = (sc_df[\"choice\"].values == \"A\").astype(float)\n",
    "    w_samp = sc_df.get(\"w\", pd.Series(1.0, index=sc_df.index)).values.astype(\"float64\")\n",
    "    Va = norm01_clipped(vA, v_lo, v_hi); Vb = norm01_clipped(vB, v_lo, v_hi)\n",
    "    Ta = norm01_clipped(tA, t_lo, t_hi); Tb = norm01_clipped(tB, t_lo, t_hi)\n",
    "    Pa = commute_penalty_shape(Ta, k_ref); Pb = commute_penalty_shape(Tb, k_ref)\n",
    "    X = np.column_stack([np.ones(len(sc_df)), (Va - Vb), -(Pa - Pb)])  # [w0,a,b]\n",
    "    w = np.array(prior, dtype=\"float64\")\n",
    "    denom = max(1.0, np.sum(w_samp))\n",
    "    for _ in range(iters):\n",
    "        z = X @ w\n",
    "        p = 1.0 / (1.0 + np.exp(-z))\n",
    "        grad_ll = X.T @ ((y - p) * w_samp) / denom\n",
    "        grad_prior = -l2 * (w - np.array(prior))\n",
    "        w += lr * (grad_ll + grad_prior)\n",
    "        w[0] = np.clip(w[0], *clip_w0); w[1] = np.clip(w[1], *clip_a); w[2] = np.clip(w[2], *clip_b)\n",
    "    w[1] = max(0.0, w[1]); w[2] = max(0.0, w[2])\n",
    "    return tuple(w)\n",
    "\n",
    "@st.cache_data(show_spinner=False)\n",
    "def build_edge_tradeoffs(_df, v_range, t_range, n=10, seed=42,\n",
    "                         dtnorm_range=(0.05, 0.18), dvnorm_range=(0.10, 0.30)):\n",
    "    \"\"\"\n",
    "    Robust scenario builder: always returns a DataFrame with\n",
    "    [qid, A_gem, A_vacancy_pct, A_travel_min, B_gem, B_vacancy_pct, B_travel_min]\n",
    "    where A and B are hard trade-offs (one better housing, the other better commute).\n",
    "    \"\"\"\n",
    "    # Coerce to DataFrame safely\n",
    "    if isinstance(_df, (pd.DataFrame, gpd.GeoDataFrame)):\n",
    "        df = _df.copy()\n",
    "    else:\n",
    "        df = pd.DataFrame(_df)\n",
    "\n",
    "    # Ensure necessary columns exist\n",
    "    need = [\"GEMEINDE_NAME\",\"vacancy_pct\",\"avg_travel_min\"]\n",
    "    for c in need:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "\n",
    "    df = df[need].copy()\n",
    "    df[\"vacancy_pct\"]    = pd.to_numeric(df[\"vacancy_pct\"], errors=\"coerce\")\n",
    "    df[\"avg_travel_min\"] = pd.to_numeric(df[\"avg_travel_min\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"GEMEINDE_NAME\",\"vacancy_pct\",\"avg_travel_min\"])\n",
    "\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\"qid\",\"A_gem\",\"A_vacancy_pct\",\"A_travel_min\",\"B_gem\",\"B_vacancy_pct\",\"B_travel_min\"])\n",
    "\n",
    "    v_lo, v_hi = v_range; t_lo, t_hi = t_range\n",
    "    df[\"v_n\"] = norm01_clipped(df[\"vacancy_pct\"].values, v_lo, v_hi)\n",
    "    df[\"t_n\"] = norm01_clipped(df[\"avg_travel_min\"].values, t_lo, t_hi)\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    used = set(); rows = []; tries = 0\n",
    "    while len(rows) < n and tries < 8000:\n",
    "        tries += 1\n",
    "        A = df.sample(1, random_state=int(rng.integers(1, 1_000_000))).iloc[0]\n",
    "        if A[\"GEMEINDE_NAME\"] in used:\n",
    "            continue\n",
    "        orientation = int(rng.integers(0, 2))\n",
    "\n",
    "        if orientation == 0:\n",
    "            # A better housing, B better commute\n",
    "            cand = df[\n",
    "                (df[\"v_n\"] < A[\"v_n\"]) &\n",
    "                (df[\"t_n\"] < A[\"t_n\"]) &\n",
    "                ((A[\"v_n\"] - df[\"v_n\"]).between(dvnorm_range[0], dvnorm_range[1], inclusive=\"both\")) &\n",
    "                ((A[\"t_n\"] - df[\"t_n\"]).between(dtnorm_range[0], dtnorm_range[1], inclusive=\"both\")) &\n",
    "                (~df[\"GEMEINDE_NAME\"].isin(used | {A[\"GEMEINDE_NAME\"]}))\n",
    "            ]\n",
    "        else:\n",
    "            # A better commute, B better housing\n",
    "            cand = df[\n",
    "                (df[\"v_n\"] > A[\"v_n\"]) &\n",
    "                (df[\"t_n\"] > A[\"t_n\"]) &\n",
    "                ((df[\"v_n\"] - A[\"v_n\"]).between(dvnorm_range[0], dvnorm_range[1], inclusive=\"both\")) &\n",
    "                ((df[\"t_n\"] - A[\"t_n\"]).between(dtnorm_range[0], dtnorm_range[1], inclusive=\"both\")) &\n",
    "                (~df[\"GEMEINDE_NAME\"].isin(used | {A[\"GEMEINDE_NAME\"]}))\n",
    "            ]\n",
    "\n",
    "        if cand.empty:\n",
    "            continue\n",
    "        B = cand.sample(1, random_state=int(rng.integers(1, 1_000_000))).iloc[0]\n",
    "        rows.append({\n",
    "            \"qid\": len(rows)+1,\n",
    "            \"A_gem\": A[\"GEMEINDE_NAME\"], \"A_vacancy_pct\": float(A[\"vacancy_pct\"]), \"A_travel_min\": float(A[\"avg_travel_min\"]),\n",
    "            \"B_gem\": B[\"GEMEINDE_NAME\"], \"B_vacancy_pct\": float(B[\"vacancy_pct\"]), \"B_travel_min\": float(B[\"avg_travel_min\"]),\n",
    "        })\n",
    "        used.add(A[\"GEMEINDE_NAME\"]); used.add(B[\"GEMEINDE_NAME\"])\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "@st.cache_data(show_spinner=False)\n",
    "def load_data():\n",
    "    gj = DATA_DIR / \"gemeinden.geojson\"\n",
    "    mj = DATA_DIR / \"meta.json\"\n",
    "    if not gj.exists() or not mj.exists():\n",
    "        st.error(\"Artifacts not found. Re-run the export step in the notebook.\")\n",
    "        st.stop()\n",
    "    g = gpd.read_file(str(gj))\n",
    "    if g.crs is None or str(g.crs).lower() != \"epsg:4326\":\n",
    "        g = g.to_crs(4326)\n",
    "    for c in [\"vacancy_pct\",\"avg_travel_min\",\"preference_score\"]:\n",
    "        if c in g.columns:\n",
    "            g[c] = pd.to_numeric(g[c], errors=\"coerce\")\n",
    "    meta = json.loads((mj).read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    tto = None\n",
    "    ttp = DATA_DIR / \"tt_by_origin.parquet\"\n",
    "    if ttp.exists():\n",
    "        try:\n",
    "            tto = pd.read_parquet(ttp)\n",
    "            tto[\"GEMEINDE_CODE\"]  = tto[\"GEMEINDE_CODE\"].astype(str)\n",
    "            tto[\"avg_travel_min\"] = pd.to_numeric(tto[\"avg_travel_min\"], errors=\"coerce\")\n",
    "            if \"origin_name\" not in tto.columns and \"origin_station_id\" in tto.columns:\n",
    "                tto[\"origin_name\"] = tto[\"origin_station_id\"].astype(str)\n",
    "        except Exception:\n",
    "            tto = None\n",
    "    return g, meta, tto\n",
    "\n",
    "# ---------- App ----------\n",
    "g, meta, tt_by_origin = load_data()\n",
    "\n",
    "st.title(\"Swiss Commute & Housing Explorer (Notebook Demo)\")\n",
    "st.markdown(\n",
    "    \"Three modes: **Preference score**, **Housing only**, **Commute only**. \"\n",
    "    \"Pick the **origin SBB station** in the **left sidebar**. \"\n",
    "    \"The Preference score blends **vacancy** (higher is better) and **commute** (shorter is better).\"\n",
    ")\n",
    "\n",
    "# ---------- Sidebar controls ----------\n",
    "with st.sidebar:\n",
    "    st.header(\"Controls\")\n",
    "    def _is_bahn2000(name: str) -> bool:\n",
    "        s = str(name).lower()\n",
    "        return (\"bahn\" in s) and (\"2000\" in s)\n",
    "\n",
    "    if tt_by_origin is not None and \"origin_name\" in tt_by_origin.columns:\n",
    "        stations_raw = tt_by_origin[\"origin_name\"].dropna().astype(str).unique().tolist()\n",
    "        stations = sorted([s for s in stations_raw if not _is_bahn2000(s)], key=str.casefold)\n",
    "        default_origin = meta.get(\"origin_station\", \"Zürich HB\")\n",
    "        idx = stations.index(default_origin) if default_origin in stations else 0\n",
    "        origin_name = st.selectbox(\"Origin SBB station\", options=stations, index=idx)\n",
    "    else:\n",
    "        origin_name = meta.get(\"origin_station\", \"Zürich HB\")\n",
    "        st.info(f\"Dynamic origins not found; using precomputed origin: **{origin_name}**\")\n",
    "\n",
    "    map_mode = st.radio(\"Map mode\", [\"Preference score\",\"Housing only\",\"Commute only\"])\n",
    "    penalty_k = st.slider(\n",
    "        \"Commute penalty curvature (k)\",\n",
    "        0.2, 6.0, float(meta.get(\"penalty_k\", 1.5)), 0.1,\n",
    "        help=\"Adjusts the curvature used in Preference score.\"\n",
    "    )\n",
    "\n",
    "# ---------- Effective commute times for selected origin (origin=0 + zero-shift) ----------\n",
    "df = g.copy()\n",
    "df[\"GEMEINDE_CODE\"] = df[\"GEMEINDE_CODE\"].astype(str)\n",
    "\n",
    "if tt_by_origin is not None and origin_name in tt_by_origin.get(\"origin_name\", pd.Series(dtype=str)).astype(str).unique():\n",
    "    tt_sel = tt_by_origin.loc[tt_by_origin[\"origin_name\"] == origin_name, [\"GEMEINDE_CODE\",\"avg_travel_min\"]].copy()\n",
    "else:\n",
    "    tt_sel = None\n",
    "\n",
    "df = enforce_origin_zero_and_shift(df, tt_sel)\n",
    "\n",
    "# Simplify for speed\n",
    "try:\n",
    "    df[\"geometry\"] = df.geometry.simplify(0.0005, preserve_topology=True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ---------- Robust ranges from *effective* data ----------\n",
    "v_range = robust_range(df[\"vacancy_pct\"])\n",
    "t_range = robust_range(df[\"avg_travel_min_eff\"])\n",
    "\n",
    "# ---------- Build scenarios (edge-case tradeoffs) ----------\n",
    "df_scen = df[[\"GEMEINDE_NAME\",\"vacancy_pct\",\"avg_travel_min_eff\"]].copy()\n",
    "df_scen = df_scen.rename(columns={\"avg_travel_min_eff\":\"avg_travel_min\"})\n",
    "sc = build_edge_tradeoffs(\n",
    "    df_scen, v_range=v_range, t_range=t_range, n=10, seed=42,\n",
    "    dtnorm_range=(0.05, 0.18), dvnorm_range=(0.10, 0.30)\n",
    ")\n",
    "\n",
    "# ---------- Tabs ----------\n",
    "tab_map, tab_pref, tab_data = st.tabs([\"🗺️ Map\",\"🧭 Preference elicitation\",\"📄 Data preview\"])\n",
    "\n",
    "# ---------- Preference elicitation (live; no button) ----------\n",
    "with tab_pref:\n",
    "    st.subheader(\"10 trade-off questions (hard choices)\")\n",
    "    if sc.empty:\n",
    "        st.warning(\"Not enough data to build scenarios.\")\n",
    "    else:\n",
    "        for _, row in sc.iterrows():\n",
    "            st.markdown(\"---\")\n",
    "            c1, c2, c3 = st.columns([1,1,1])\n",
    "            with c1:\n",
    "                st.markdown(f\"**Q{int(row.qid)} – Option A**\")\n",
    "                st.write(row.A_gem)\n",
    "                st.write(f\"Vacancy: {row.A_vacancy_pct:.2f}%\")\n",
    "                st.write(f\"Commute: {row.A_travel_min:.1f} min\")\n",
    "            with c2:\n",
    "                st.markdown(f\"**Q{int(row.qid)} – Option B**\")\n",
    "                st.write(row.B_gem)\n",
    "                st.write(f\"Vacancy: {row.B_vacancy_pct:.2f}%\")\n",
    "                st.write(f\"Commute: {row.B_travel_min:.1f} min\")\n",
    "            with c3:\n",
    "                st.radio(\n",
    "                    f\"Your choice for Q{int(row.qid)}\",\n",
    "                    [\"A\",\"B\"], key=f\"choice_{int(row.qid)}\",\n",
    "                    horizontal=True, index=None\n",
    "                )\n",
    "\n",
    "# ---------- Learn weights only when answers change (k_ref fixed) ----------\n",
    "def answers_signature(choices_dict):\n",
    "    key_order = sorted(choices_dict.keys())\n",
    "    s = \"|\".join(f\"{k}:{choices_dict[k]}\" for k in key_order)\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "choices_dict, answered_rows = {}, []\n",
    "for _, r in sc.iterrows():\n",
    "    key = f\"choice_{int(r.qid)}\"\n",
    "    ch = st.session_state.get(key, None)\n",
    "    if ch in (\"A\",\"B\"):\n",
    "        choices_dict[key] = ch\n",
    "        answered_rows.append(r)\n",
    "\n",
    "ans_sig = answers_signature(choices_dict) if choices_dict else None\n",
    "\n",
    "if answered_rows:\n",
    "    user_sc = pd.DataFrame(answered_rows).reset_index(drop=True).copy()\n",
    "    user_sc[\"choice\"] = [choices_dict[f\"choice_{int(r.qid)}\"] for _, r in user_sc.iterrows()]\n",
    "    user_sc[\"w\"] = 1.0\n",
    "    anchors = build_anchor_pairs(v_range, t_range, n_each=3, dv_n=0.18, dt_n=0.10)\n",
    "    train_df = pd.concat([\n",
    "        user_sc[[\"A_vacancy_pct\",\"A_travel_min\",\"B_vacancy_pct\",\"B_travel_min\",\"choice\",\"w\"]],\n",
    "        anchors[[\"A_vacancy_pct\",\"A_travel_min\",\"B_vacancy_pct\",\"B_travel_min\",\"choice\",\"w\"]],\n",
    "    ], ignore_index=True)\n",
    "else:\n",
    "    train_df = None\n",
    "\n",
    "if train_df is not None and st.session_state.get(\"answers_sig\") != ans_sig:\n",
    "    prior = resolve_prior(meta, st.session_state)\n",
    "    try:\n",
    "        w0, a, b = fit_logistic_weighted(\n",
    "            train_df, v_range=v_range, t_range=t_range, k_ref=1.0,\n",
    "            prior=prior, l2=0.25, iters=900, lr=0.25\n",
    "        )\n",
    "        st.session_state[\"manual_weights\"] = {\n",
    "            \"w0\": float(w0), \"w_v\": float(a), \"w_t\": float(b),\n",
    "            \"v_min\": float(v_range[0]), \"v_max\": float(v_range[1]),\n",
    "            \"t_min\": float(t_range[0]), \"t_max\": float(t_range[1]),\n",
    "        }\n",
    "        st.session_state[\"answers_sig\"] = ans_sig\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ---------- Data preview ----------\n",
    "with tab_data:\n",
    "    show = df.copy()\n",
    "    show[\"avg_travel_min_eff\"] = pd.to_numeric(show[\"avg_travel_min_eff\"], errors=\"coerce\")\n",
    "    cols = [\"GEMEINDE_NAME\",\"vacancy_pct\",\"avg_travel_min_eff\",\"preference_score\"]\n",
    "    cols = [c for c in cols if c in show.columns]\n",
    "    st.dataframe(show[cols].head(30).rename(columns={\"avg_travel_min_eff\":\"avg_travel_min\"}), use_container_width=True)\n",
    "    st.caption(f\"Total rows: {len(show)}\")\n",
    "\n",
    "# ---------- Map ----------\n",
    "with tab_map:\n",
    "    if map_mode == \"Housing only\":\n",
    "        metric_col = \"vacancy_pct\"; legend = \"Vacancy rate (%)\"; colors = [\"red\",\"yellow\",\"green\"]\n",
    "        vals = pd.to_numeric(df[\"vacancy_pct\"], errors=\"coerce\")\n",
    "        vmin = float(np.nanpercentile(vals, 1)) if vals.notna().any() else 0.0\n",
    "        vmax = float(np.nanpercentile(vals, 99)) if vals.notna().any() else 1.0\n",
    "        df_render = df.copy()\n",
    "\n",
    "    elif map_mode == \"Commute only\":\n",
    "        metric_col = \"avg_travel_min_eff\"; legend = \"Commute time from origin (min)\"; colors = [\"green\",\"yellow\",\"red\"]\n",
    "        vals = pd.to_numeric(df[\"avg_travel_min_eff\"], errors=\"coerce\")\n",
    "        vmin = 0.0\n",
    "        vmax = float(np.nanpercentile(vals, 99)) if vals.notna().any() else 120.0\n",
    "        df_render = df.dropna(subset=[metric_col]).copy()\n",
    "\n",
    "    else:\n",
    "        metric_col = \"preference_score\"; legend = \"Preference score (0–100)\"; colors = [\"red\",\"yellow\",\"green\"]\n",
    "\n",
    "        if \"manual_weights\" in st.session_state:\n",
    "            w0 = float(st.session_state[\"manual_weights\"].get(\"w0\", 0.0))\n",
    "            a  = float(st.session_state[\"manual_weights\"].get(\"w_v\", 2.0))\n",
    "            b  = float(st.session_state[\"manual_weights\"].get(\"w_t\", 2.0))\n",
    "            v_lo, v_hi = float(st.session_state[\"manual_weights\"].get(\"v_min\", v_range[0])), float(st.session_state[\"manual_weights\"].get(\"v_max\", v_range[1]))\n",
    "            t_lo, t_hi = float(st.session_state[\"manual_weights\"].get(\"t_min\", t_range[0])), float(st.session_state[\"manual_weights\"].get(\"t_max\", t_range[1]))\n",
    "        else:\n",
    "            w0, a, b = resolve_prior(meta, st.session_state)\n",
    "            v_lo, v_hi = v_range; t_lo, t_hi = t_range\n",
    "\n",
    "        v_all = norm01_clipped(pd.to_numeric(df[\"vacancy_pct\"], errors=\"coerce\").values, v_lo, v_hi)\n",
    "        t_all = norm01_clipped(pd.to_numeric(df[\"avg_travel_min_eff\"], errors=\"coerce\").values, t_lo, t_hi)\n",
    "        pen_t = commute_penalty_shape(t_all, penalty_k)\n",
    "        util  = (w0 + a * v_all - b * pen_t)\n",
    "\n",
    "        finite = np.isfinite(util)\n",
    "        if finite.any():\n",
    "            util = util - np.median(util[finite])\n",
    "\n",
    "        df_render = df.copy()\n",
    "        df_render[\"preference_score\"] = 100.0 * (1.0 / (1.0 + np.exp(-util)))\n",
    "        vmin, vmax = 0.0, 100.0\n",
    "        df_render = df_render.dropna(subset=[\"preference_score\"])\n",
    "\n",
    "    # Folium renderer\n",
    "    def render_folium(df_map):\n",
    "        import folium, branca.colormap as bcm\n",
    "        from streamlit_folium import st_folium\n",
    "        import streamlit.components.v1 as components\n",
    "        colormap = bcm.LinearColormap(colors=colors, vmin=vmin, vmax=vmax); colormap.caption = legend\n",
    "        m = folium.Map(location=[46.8, 8.2], zoom_start=7, tiles=\"OpenStreetMap\")\n",
    "\n",
    "        if metric_col == \"vacancy_pct\":\n",
    "            df_map[\"vacancy_round\"] = pd.to_numeric(df_map[\"vacancy_pct\"], errors=\"coerce\").round(2)\n",
    "            fields, aliases = [\"GEMEINDE_NAME\",\"vacancy_round\"], [\"Municipality\",\"Vacancy (%)\"]\n",
    "        elif metric_col == \"avg_travel_min_eff\":\n",
    "            df_map[\"time_round\"] = pd.to_numeric(df_map[\"avg_travel_min_eff\"], errors=\"coerce\").round(1)\n",
    "            fields, aliases = [\"GEMEINDE_NAME\",\"time_round\"], [\"Municipality\",\"Commute (min)\"]\n",
    "        else:\n",
    "            df_map[\"score_round\"] = pd.to_numeric(df_map[\"preference_score\"], errors=\"coerce\").round(1)\n",
    "            df_map[\"vacancy_round\"] = pd.to_numeric(df_map[\"vacancy_pct\"], errors=\"coerce\").round(2)\n",
    "            df_map[\"time_round\"] = pd.to_numeric(df_map[\"avg_travel_min_eff\"], errors=\"coerce\").round(1)\n",
    "            fields, aliases = [\"GEMEINDE_NAME\",\"score_round\",\"vacancy_round\",\"time_round\"], [\"Municipality\",\"Score\",\"Vacancy (%)\",\"Commute (min)\"]\n",
    "\n",
    "        folium.GeoJson(\n",
    "            data=df_map.to_json(),\n",
    "            style_function=lambda feat: {\n",
    "                \"fillColor\": colormap(float(feat[\"properties\"].get(metric_col))) if feat[\"properties\"].get(metric_col) is not None else \"#cccccc\",\n",
    "                \"color\": \"white\", \"weight\": 0.2, \"fillOpacity\": 0.85\n",
    "            },\n",
    "            tooltip=folium.features.GeoJsonTooltip(fields=fields, aliases=aliases, localize=True)\n",
    "        ).add_to(m)\n",
    "        colormap.add_to(m)\n",
    "\n",
    "        try:\n",
    "            st_folium(m, height=720, width=None)\n",
    "        except Exception:\n",
    "            html = m._repr_html_()\n",
    "            components.html(html, height=720, scrolling=False)\n",
    "\n",
    "    if df_render.empty:\n",
    "        st.warning(\"No municipalities available to render for this selection.\")\n",
    "    else:\n",
    "        render_folium(df_render)\n",
    "'''\n",
    "\n",
    "with open(APP_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(app_code)\n",
    "\n",
    "print(\"✅ Wrote app.py (robust scenarios + origin=0 + zero-shift)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43260,
     "status": "ok",
     "timestamp": 1759138622689,
     "user": {
      "displayName": "Ingo Stallknecht",
      "userId": "04299073505858791564"
     },
     "user_tz": -120
    },
    "id": "5TA18rFPi4hE",
    "outputId": "77146c56-2958-49a7-c0a9-a63bc5076cbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "^C\n",
      "⏳ Waiting for Streamlit...\n",
      "✅ Streamlit up at http://127.0.0.1:8501\n",
      "^C\n",
      "⚠️  Tunnel attempt 1 failed; retrying...\n",
      "^C\n",
      "\n",
      "🌐 Your Streamlit app is live at: https://venues-exploration-estimates-programmers.trycloudflare.com\n",
      "\n",
      "--- streamlit.log tail ---\n",
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
      "\n",
      "\n",
      "  You can now view your Streamlit app in your browser.\n",
      "\n",
      "  Local URL: http://localhost:8501\n",
      "  Network URL: http://172.28.0.12:8501\n",
      "  External URL: http://34.41.110.118:8501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Launch Streamlit with Cloudflare tunnel ---\n",
    "import os, re, time, subprocess\n",
    "\n",
    "APP_DIR = \"/content/streamlit_app\"\n",
    "APP_PATH = os.path.join(APP_DIR, \"app.py\")\n",
    "CF_BIN  = \"/content/cloudflared\"\n",
    "CF_LOG  = \"/content/cflog.txt\"\n",
    "ST_LOG  = \"/content/streamlit.log\"\n",
    "\n",
    "# Deps\n",
    "!pip -q install streamlit streamlit-folium geopandas folium branca shapely pyproj rtree pydeck >/dev/null\n",
    "\n",
    "# Kill stale procs\n",
    "!pkill -f \"cloudflared\" || true\n",
    "!pkill -f \"streamlit run\" || true\n",
    "\n",
    "# Start Streamlit\n",
    "st_out = open(ST_LOG, \"w\")\n",
    "st_proc = subprocess.Popen(\n",
    "    [\"streamlit\", \"run\", APP_PATH, \"--server.port=8501\", \"--server.headless=true\"],\n",
    "    cwd=APP_DIR, stdout=st_out, stderr=subprocess.STDOUT, text=True\n",
    ")\n",
    "\n",
    "# Wait for healthz\n",
    "def wait_for_streamlit(timeout=45):\n",
    "    start = time.time()\n",
    "    while time.time() - start < timeout:\n",
    "        ok = subprocess.run([\"curl\", \"-sS\", \"-m\", \"2\", \"http://127.0.0.1:8501/healthz\"],\n",
    "                            capture_output=True, text=True)\n",
    "        if ok.returncode == 0 and \"ok\" in ok.stdout.lower():\n",
    "            return True\n",
    "        time.sleep(1)\n",
    "    return False\n",
    "\n",
    "print(\"⏳ Waiting for Streamlit...\")\n",
    "if not wait_for_streamlit():\n",
    "    print(\"--- streamlit.log tail ---\")\n",
    "    !tail -n 80 \"/content/streamlit.log\"\n",
    "    raise RuntimeError(\"Streamlit didn't become ready.\")\n",
    "\n",
    "print(\"✅ Streamlit up at http://127.0.0.1:8501\")\n",
    "\n",
    "# Cloudflare tunnel\n",
    "!curl -sL -o \"{CF_BIN}\" https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
    "!chmod +x \"{CF_BIN}\"\n",
    "\n",
    "def start_tunnel():\n",
    "    subprocess.Popen([CF_BIN, \"tunnel\", \"--url\", \"http://127.0.0.1:8501\", \"--no-autoupdate\"],\n",
    "                     stdout=open(CF_LOG, \"w\"), stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "def find_url(deadline=25):\n",
    "    pat = re.compile(r\"https://[-a-z0-9]+\\.trycloudflare\\.com\")\n",
    "    start = time.time()\n",
    "    while time.time() - start < deadline:\n",
    "        try:\n",
    "            with open(CF_LOG, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                log = f.read()\n",
    "            m = pat.search(log)\n",
    "            if m:\n",
    "                return m.group(0)\n",
    "        except Exception:\n",
    "            pass\n",
    "        time.sleep(1)\n",
    "    return None\n",
    "\n",
    "public_url = None\n",
    "for attempt in range(1, 3):\n",
    "    !pkill -f \"cloudflared\" || true\n",
    "    start_tunnel()\n",
    "    time.sleep(2)\n",
    "    url = find_url(25)\n",
    "    if url:\n",
    "        public_url = url\n",
    "        break\n",
    "    print(f\"⚠️  Tunnel attempt {attempt} failed; retrying...\")\n",
    "\n",
    "if not public_url:\n",
    "    print(\"--- cloudflared log tail ---\")\n",
    "    !tail -n 120 \"{CF_LOG}\"\n",
    "    raise RuntimeError(\"Cloudflare tunnel did not attach. Re-run this cell.\")\n",
    "\n",
    "print(\"\\n🌐 Your Streamlit app is live at:\", public_url)\n",
    "print(\"\\n--- streamlit.log tail ---\")\n",
    "!tail -n 60 \"/content/streamlit.log\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 249,
     "status": "ok",
     "timestamp": 1759138622942,
     "user": {
      "displayName": "Ingo Stallknecht",
      "userId": "04299073505858791564"
     },
     "user_tz": -120
    },
    "id": "GL0EmX7WgfJ7",
    "outputId": "8b41a065-9898-4417-d90d-08d002ceb0a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "^C\n",
      "Stopped Streamlit and Cloudflare tunnel.\n"
     ]
    }
   ],
   "source": [
    "# ===== 13) Stop Streamlit & Cloudflare =====\n",
    "!pkill -f \"cloudflared\" || true\n",
    "!pkill -f \"streamlit run\" || true\n",
    "print(\"Stopped Streamlit and Cloudflare tunnel.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPH+GvsSUUicITexevu1MrM",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
